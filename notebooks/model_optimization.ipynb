{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/envRapidSOH/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('<path to cloned git repository folder>/rapid-soh-estimation-from-short-pulses')\n",
    "\n",
    "from rapid_soh_estimation.rapid_soh_estimation.config import *\n",
    "from rapid_soh_estimation.rapid_soh_estimation.common_methods import *\n",
    "\n",
    "\n",
    "cc_data = load_processed_data(data_type='cc')\n",
    "pulse_data = load_processed_data(data_type='slowpulse')\n",
    "\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_hlayers:int, n_neurons:int, act_fnc:str, opt_fnc:str, learning_rate:float, input_shape=(100,), output_shape=(7,)) -> keras.models.Sequential:\n",
    "\t\"\"\"Builds a Keras neural network model (MLP) using the specified parameters. The model is optimized for accuracy. Make sure model outputs (if multiple target) are normalized, otherwise optimization will be biased towards one target variable.\n",
    "\n",
    "\tArgs:\n",
    "\t\tn_hlayers (int): Number of fully-connected hidden layers\n",
    "\t\tn_neurons (int): Number of neurons per hidden layer\n",
    "\t\tact_fnc (str): Activation function to use (\\'tanh\\', \\'relu\\', etc)\n",
    "\t\topt_fnc (str): {\\'sgd\\', \\'adam\\'} Optimizer function to use \n",
    "\t\tlearning_rate (float): Learning rate\n",
    "\t\tinput_shape (int, optional): Input shape of model. Defaults to (100,).\n",
    "\t\toutput_shape (int, optional): Output shape of model. Default to (7,).\n",
    "\tRaises:\n",
    "\t\tValueError: _description_\n",
    "\n",
    "\tReturns:\n",
    "\t\tkeras.models.Sequential: compiled Keras model\n",
    "\t\"\"\"\n",
    "\n",
    "\t# add input layer to Sequential model\n",
    "\tmodel = keras.models.Sequential()\n",
    "\tmodel.add( keras.Input(shape=input_shape) )\n",
    "\n",
    "\t# add hidden layers\n",
    "\tfor i in range(n_hlayers):\n",
    "\t\tmodel.add( keras.layers.Dense(units=n_neurons, activation=act_fnc) )\n",
    "\t\t\n",
    "\t# add output layer\n",
    "\tmodel.add( keras.layers.Dense(output_shape) )\n",
    "\n",
    "\t# compile model with chosen metrics\n",
    "\topt = None\n",
    "\tif opt_fnc == 'adam':\n",
    "\t\topt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\telif opt_fnc == 'sgd':\n",
    "\t\topt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\telse:\n",
    "\t\traise ValueError(\"opt_func must be either \\'adam\\' or \\'sgd\\'\")\n",
    "\n",
    "\tmodel.compile(\n",
    "\t\toptimizer=opt,\n",
    "\t\tloss=keras.losses.mean_squared_error,      \n",
    "\t\t# make sure to normalize all outputs, otherwise DCIR values will drastically skew MSE reading compared to error of predicted SOH\n",
    "\t\tmetrics=['accuracy'] )\n",
    "\treturn model\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "\t\"\"\"The loss function used for all optuna studies\n",
    "\n",
    "\tArgs:\n",
    "\t\ty_true (MatrixLike | ArrayLike): Ground truth (correct) target values\n",
    "\t\ty_pred (MatrixLike | ArrayLike): Estimated target values\n",
    "\n",
    "\tReturns:\n",
    "\t\t_type_: The loss for this prediction (ie, prediction error)\n",
    "\t\"\"\"\n",
    "\n",
    "\tloss = mean_squared_error(y_true, y_pred)\n",
    "\treturn loss\n",
    "\n",
    "class OptunaHyperParamOptimization:\n",
    "\tdef __init__(self, X, y, splits, loss_fnc, random_state):\n",
    "\t\tself.X = X\n",
    "\t\tif len(self.X.shape) == 1: self.X = self.X.reshape(-1,1)\n",
    "\t\tself.y = y\n",
    "\t\tif len(self.y.shape) == 1: self.y = self.y.reshape(-1,1)\n",
    "\t\tself.splits = splits\n",
    "\t\tself.loss_fnc = loss_fnc\n",
    "\t\tself.random_state = random_state\n",
    "\n",
    "\tdef __call__(self, trial:optuna.trial.Trial):\n",
    "\t\treturn None\n",
    "\n",
    "class OptunaHyperParamOptimization_MLP(OptunaHyperParamOptimization):\n",
    "\tdef __call__(self, trial:optuna.trial.Trial):\n",
    "\t\t# average the loss over all cross-validation splits\n",
    "\t\ttotal_loss = 0\n",
    "\t\titer_count = 0\n",
    "\t\tfor train_idxs, test_idxs in self.splits:\n",
    "\t\t\t# get suggested parameter values from Optuna search space\n",
    "\t\t\tn_hlayers = trial.suggest_int('n_hlayers', 1, 5)\n",
    "\t\t\tn_neurons = trial.suggest_int('n_neurons', 4, 100)\n",
    "\t\t\tact_fnc = trial.suggest_categorical('act_fnc', ['linear','relu','sigmoid','softmax','softplus','tanh'])\n",
    "\t\t\topt_fnc = trial.suggest_categorical('opt_fnc', ['adam', 'sgd'])\n",
    "\t\t\tlearning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "\n",
    "\t\t\t# create sequential NN model\n",
    "\t\t\tmodel = create_model(\n",
    "\t\t\t\tn_hlayers=n_hlayers,\n",
    "\t\t\t\tn_neurons=n_neurons,\n",
    "\t\t\t\tact_fnc=act_fnc,\n",
    "\t\t\t\topt_fnc=opt_fnc,\n",
    "\t\t\t\tlearning_rate=learning_rate,\n",
    "\t\t\t\tinput_shape=self.X.shape[1],\n",
    "\t\t\t\toutput_shape=self.y.shape[1]\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# standardize input and output data (using only the training data to create the scaler)\n",
    "\t\t\tscaler_X = StandardScaler().fit(self.X[train_idxs])\n",
    "\t\t\tscaler_y = StandardScaler().fit(self.y[train_idxs])\n",
    "\t\t\tX_sc = scaler_X.transform(self.X)\n",
    "\t\t\ty_sc = scaler_y.transform(self.y)\n",
    "   \n",
    "\t\t\t# fit model to scaled input and output data\n",
    "\t\t\tearly_stop = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=False, mode='auto', baseline=None, restore_best_weights=True)\n",
    "\t\t\tmodel.fit(\n",
    "\t\t\t\tX_sc[train_idxs], \n",
    "\t\t\t\ty_sc[train_idxs],\n",
    "\t\t\t\tcallbacks = early_stop, \n",
    "\t\t\t\tverbose = 0)\n",
    "\t\t\t\n",
    "\t\t\t# get predictions\n",
    "\t\t\ty_pred_sc = model.predict(X_sc[test_idxs], verbose=0)\n",
    "\n",
    "\t\t\t# check if nan value exists in predictions (model failed to converge during traing)\n",
    "\t\t\tif not (len(np.where(y_pred_sc == np.nan)[0]) == 0) or not (len(np.where(y_pred_sc == None)[0]) == 0):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# add loss to total\n",
    "\t\t\ttotal_loss += self.loss_fnc(y_sc[test_idxs], y_pred_sc)\n",
    "\t\t\titer_count += 1\n",
    "\n",
    "\t\t# return average cross-validation loss\n",
    "\t\treturn total_loss / iter_count\n",
    "\n",
    "def print_optuna_study_results(study:optuna.Study):\n",
    "\t\"\"\"Prints the best loss and parameters for a given Optuna study\"\"\"\n",
    "\t\n",
    "\tprint()\n",
    "\tprint('*'*100)\n",
    "\tprint(f'  Study: {study.study_name}')\n",
    "\tprint('*'*100)\n",
    "\tprint('  Best Loss: ', study.best_trial.value)\n",
    "\tprint('  Best Params: ')\n",
    "\tfor k,v in study.best_trial.params.items():\n",
    "\t\tprint(f'    {k}: {v}')\n",
    "\tprint('*'*100)\n",
    "\tprint()\n",
    "\n",
    "def perform_hyperparam_optimization(params:dict, dir_results:Path):\n",
    "\tfor key,val in params.items():\n",
    "\t\tstudy = optuna.create_study(\n",
    "\t\t\tstudy_name=key,\n",
    "\t\t\tdirection='minimize', \n",
    "\t\t\tsampler=optuna.samplers.TPESampler(seed=random_state),)\n",
    "\t\tstudy.optimize(\n",
    "\t\t\tfunc = val['objective'],\n",
    "\t\t\tn_trials = val['n_trials'],\n",
    "\t\t\tn_jobs=-1)\n",
    "\t\tif val['save_results']:\n",
    "\t\t\tpickle.dump(\n",
    "\t\t\t\tstudy, \n",
    "\t\t\t\topen(dir_results.joinpath(f\"{val['filename']}.pkl\"), 'wb'),\n",
    "\t\t\t\tprotocol=pickle.HIGHEST_PROTOCOL)\n",
    "\t\telse:\n",
    "\t\t\tprint_optuna_study_results(study)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Model for Different Input Types (chg vs. dchg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = deepcopy(pulse_data)\n",
    "idxs = np.where((all_data['pulse_type'] == 'chg') & (all_data['soc'] == 20))\n",
    "for k in all_data.keys():\n",
    "\tall_data[k] = all_data[k][idxs]\n",
    "all_data['voltage_rel'] = np.asarray([v - v[0] for v in all_data['voltage']])\n",
    "modeling_data = create_modeling_data(all_data=all_data, input_feature_keys=['voltage_rel'])\n",
    "modeling_data['model_output'] = modeling_data['model_output'][:,0].reshape(-1,1)\n",
    "\n",
    "random_state = 13\n",
    "cv_splitter = Custom_CVSplitter(n_splits=3, split_type='group_id', rand_seed=random_state)\n",
    "cv_splits = list(cv_splitter.split(\n",
    "\tX = modeling_data['model_input'], \n",
    "\ty = modeling_data['model_output'], \n",
    "\tcell_ids = modeling_data['cell_id']))\n",
    "\n",
    "# key = study_name\n",
    "hyperparam_opt_params = {\n",
    "\t'Hyperparameters_MLP_chg_onlySOH': {\n",
    "\t\t'objective':\n",
    "\t\t\tOptunaHyperParamOptimization_MLP(\n",
    "\t\t\t\tX=modeling_data['model_input'], \n",
    "\t\t\t\ty=modeling_data['model_output'], \n",
    "\t\t\t\tsplits=cv_splits, \n",
    "\t\t\t\tloss_fnc=loss_function, \n",
    "\t\t\t\trandom_state=random_state\n",
    "\t\t\t),\n",
    "\t\t'n_trials':1000,\n",
    "\t\t'save_results':True,\n",
    "\t\t'filename':'hyperparam_study_mlp_chg_onlySOH'\n",
    "\t},\n",
    "}\n",
    "\n",
    "dir_hyperparam_results = dir_repo_main.joinpath(\"results\", 'hyperparameter_optimization')\n",
    "dir_hyperparam_results.mkdir(exist_ok=True, parents=True)\n",
    "# perform_hyperparam_optimization(hyperparam_opt_params, dir_hyperparam_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_type in ['chg', 'dchg']: \n",
    "    for soc in [20,50,90]:\n",
    "        # get modeling data (charge pulse only, all SOCs)\n",
    "        all_data = deepcopy(pulse_data)\n",
    "        idxs = np.where((all_data['pulse_type'] == p_type) & (all_data['soc'] == soc))\n",
    "        for k in all_data.keys():\n",
    "            all_data[k] = all_data[k][idxs]\n",
    "        all_data['voltage_rel'] = np.asarray([v - v[0] for v in all_data['voltage']])\n",
    "        modeling_data = create_modeling_data(all_data=all_data, input_feature_keys=['voltage_rel'])\n",
    "\n",
    "\n",
    "        random_state = 13\n",
    "        cv_splitter = Custom_CVSplitter(n_splits=3, split_type='group_id', rand_seed=random_state)\n",
    "        cv_splits = list(cv_splitter.split(\n",
    "            X = modeling_data['model_input'], \n",
    "            y = modeling_data['model_output'], \n",
    "            cell_ids = modeling_data['cell_id']))\n",
    "\n",
    "        # key = study_name\n",
    "        hyperparam_opt_params = {\n",
    "            f'Hyperparameters_MLP_{p_type}_{soc}': {\n",
    "                'objective':\n",
    "                    OptunaHyperParamOptimization_MLP(\n",
    "                        X=modeling_data['model_input'], \n",
    "                        y=modeling_data['model_output'], \n",
    "                        splits=cv_splits, \n",
    "                        loss_fnc=loss_function, \n",
    "                        random_state=random_state\n",
    "                    ),\n",
    "                'n_trials':1000,\n",
    "                'save_results':True,\n",
    "                'filename':f'hyperparam_study_mlp_{p_type}_{soc}'\n",
    "            },\n",
    "        }\n",
    "\n",
    "        dir_hyperparam_results = dir_repo_main.joinpath(\"results\", 'hyperparameter_optimization')\n",
    "        dir_hyperparam_results.mkdir(exist_ok=True, parents=True)\n",
    "        # perform_hyperparam_optimization(hyperparam_opt_params, dir_hyperparam_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalute Model Performance on OOD Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulse_type = 'chg'\n",
    "random_state = 1\n",
    "dir_hyperparam_results = dir_repo_main.joinpath(\"results\", 'hyperparameter_optimization')\n",
    "\n",
    "for train_soc, test_socs in {20:[50,90], 50:[20,90], 90:[20,50]}.items():\n",
    "    for file_study in dir_hyperparam_results.glob(f'*_mlp_{pulse_type}_{train_soc}.pkl'):\n",
    "        # load optimal model from optimization results for this training SOC and pulse type\n",
    "        study = pickle.load(open(file_study, 'rb'))\n",
    "        model = create_model(**study.best_trial.params, output_shape=7)\n",
    "        \n",
    "        #region: get training data\n",
    "        training_data = deepcopy(pulse_data)\n",
    "        idxs = np.where((training_data['pulse_type'] == pulse_type) & (training_data['soc'] == train_soc))\n",
    "        for k in training_data.keys():\n",
    "            training_data[k] = training_data[k][idxs]\n",
    "        training_data['voltage_rel'] = np.asarray([v - v[0] for v in training_data['voltage']])\n",
    "        training_data = create_modeling_data(all_data=training_data, input_feature_keys=['voltage_rel'])\n",
    "        #endregion\n",
    "        \n",
    "        #region: get train/test splits\n",
    "        cv_splitter = Custom_CVSplitter(n_splits=3, split_type='group_id', rand_seed=random_state)\n",
    "        cv_splits = list(cv_splitter.split(\n",
    "            X = training_data['model_input'], \n",
    "            y = training_data['model_output'], \n",
    "            cell_ids = training_data['cell_id']))\n",
    "        #endregion\n",
    "\n",
    "\n",
    "        train_err_total = 0\n",
    "        test_err_total = np.zeros(len(test_socs))\n",
    "        for train_idxs, test_idxs in cv_splits:\n",
    "            #region: fit model to data\n",
    "            # standardize input and output data (using only the training data to create the scaler)\n",
    "            scaler_X = StandardScaler().fit(training_data['model_input'][train_idxs])\n",
    "            scaler_y = StandardScaler().fit(training_data['model_output'][train_idxs])\n",
    "            training_data['model_input_scaled'] = scaler_X.transform(training_data['model_input'])\n",
    "            training_data['model_output_scaled'] = scaler_y.transform(training_data['model_output'])\n",
    "        \n",
    "            # fit model to scaled input and output data\n",
    "            early_stop = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=False, mode='auto', baseline=None, restore_best_weights=True)\n",
    "            model.fit(\n",
    "                training_data['model_input_scaled'][train_idxs], \n",
    "                training_data['model_output_scaled'][train_idxs],\n",
    "                callbacks = early_stop, \n",
    "                validation_split=0.1,\n",
    "                verbose = 0)\n",
    "            #endregion\n",
    "        \n",
    "            # get predictions\n",
    "            train_yhat = model.predict(training_data['model_input_scaled'][test_idxs], verbose=0)\n",
    "            train_err = mean_absolute_percentage_error(training_data['model_output_scaled'][test_idxs], train_yhat)\n",
    "            train_err_total += train_err\n",
    "            \n",
    "            for i, test_soc in enumerate(test_socs):\n",
    "                #region: get test data\n",
    "                test_data = deepcopy(pulse_data)\n",
    "                idxs = np.where((test_data['pulse_type'] == pulse_type) & (test_data['soc'] == test_soc))\n",
    "                for k in test_data.keys():\n",
    "                    test_data[k] = test_data[k][idxs]\n",
    "                test_data['voltage_rel'] = np.asarray([v - v[0] for v in test_data['voltage']])\n",
    "                test_data = create_modeling_data(all_data=test_data, input_feature_keys=['voltage_rel'])\n",
    "                #endregion\n",
    "                \n",
    "                test_data['model_input_scaled'] = scaler_X.transform(test_data['model_input'])\n",
    "                test_data['model_output_scaled'] = scaler_y.transform(test_data['model_output'])\n",
    "                \n",
    "                test_yhat = model.predict(test_data['model_input_scaled'][test_idxs], verbose=0)\n",
    "                test_err = mean_absolute_percentage_error(test_data['model_output_scaled'][test_idxs], test_yhat)\n",
    "                test_err_total[i] += test_err\n",
    "                \n",
    "                # print(f\"Train SOC: {train_soc}\\tTest SOC: {test_soc}\")\n",
    "                # print(f\"  Train Error: {round(train_err, 4)}\\tTest Error: {round(test_err, 4)}\")\n",
    "                \n",
    "        train_err_total /= len(cv_splits)\n",
    "        for i in range(len(test_err_total)):\n",
    "            test_err_total[i] /= len(cv_splits)\n",
    "            \n",
    "        print(f\"Train SOC: {train_soc}\\t Train Error: {round(train_err_total, 4)}\")\n",
    "        for i, test_soc in enumerate(test_socs):\n",
    "            print(f\"  Test SOC: {test_soc}\\t Test Error: {round(test_err_total[i], 4)}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Optimal NN Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapid_soh_estimation.rapid_soh_estimation.plotting import plot_optimal_NN\n",
    "\n",
    "plot_optimal_NN(use_log_norm=True, bias=0.8, save=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envRapidSOH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
