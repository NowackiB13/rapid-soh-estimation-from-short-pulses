{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/bnowacki/Documents/Git Repositories/rapid-soh-estimation-from-short-pulses')\n",
    "\n",
    "from rapid_soh_estimation.rapid_soh_estimation.config import *\n",
    "from rapid_soh_estimation.rapid_soh_estimation.common_methods import *\n",
    "\n",
    "\n",
    "cc_data = load_processed_data(data_type='cc')\n",
    "pulse_data = load_processed_data(data_type='slowpulse')\n",
    "\n",
    "\n",
    "# get modeling data (charge pulse only, all SOCs)\n",
    "all_data = deepcopy(pulse_data)\n",
    "idxs = np.where((all_data['pulse_type'] == 'chg'))\n",
    "for k in all_data.keys():\n",
    "\tall_data[k] = all_data[k][idxs]\n",
    "all_data['voltage_rel'] = np.asarray([v - v[0] for v in all_data['voltage']])\n",
    "modeling_data = create_modeling_data(all_data=all_data, input_feature_keys=['voltage_rel'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_hlayers:int, n_neurons:int, act_fnc:str, opt_fnc:str, learning_rate:float, input_shape=(100,), output_shape=(7,)) -> keras.models.Sequential:\n",
    "\t\"\"\"Builds a Keras neural network model (MLP) using the specified parameters. The model is optimized for accuracy. Make sure model outputs (if multiple target) are normalized, otherwise optimization will be biased towards one target variable.\n",
    "\n",
    "\tArgs:\n",
    "\t\tn_hlayers (int): Number of fully-connected hidden layers\n",
    "\t\tn_neurons (int): Number of neurons per hidden layer\n",
    "\t\tact_fnc (str): Activation function to use (\\'tanh\\', \\'relu\\', etc)\n",
    "\t\topt_fnc (str): {\\'sgd\\', \\'adam\\'} Optimizer function to use \n",
    "\t\tlearning_rate (float): Learning rate\n",
    "\t\tinput_shape (int, optional): Input shape of model. Defaults to (100,).\n",
    "\t\toutput_shape (int, optional): Output shape of model. Default to (7,).\n",
    "\tRaises:\n",
    "\t\tValueError: _description_\n",
    "\n",
    "\tReturns:\n",
    "\t\tkeras.models.Sequential: compiled Keras model\n",
    "\t\"\"\"\n",
    "\n",
    "\t# add input layer to Sequential model\n",
    "\tmodel = keras.models.Sequential()\n",
    "\tmodel.add( keras.Input(shape=input_shape) )\n",
    "\n",
    "\t# add hidden layers\n",
    "\tfor i in range(n_hlayers):\n",
    "\t\tmodel.add( keras.layers.Dense(units=n_neurons, activation=act_fnc) )\n",
    "\t\t\n",
    "\t# add output layer\n",
    "\tmodel.add( keras.layers.Dense(output_shape) )\n",
    "\n",
    "\t# compile model with chosen metrics\n",
    "\topt = None\n",
    "\tif opt_fnc == 'adam':\n",
    "\t\topt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\telif opt_fnc == 'sgd':\n",
    "\t\topt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\telse:\n",
    "\t\traise ValueError(\"opt_func must be either \\'adam\\' or \\'sgd\\'\")\n",
    "\n",
    "\tmodel.compile(\n",
    "\t\toptimizer=opt,\n",
    "\t\tloss=keras.losses.mean_squared_error,      \n",
    "\t\t# make sure to normalize all outputs, otherwise DCIR values will drastically skew MSE reading compared to error of predicted SOH\n",
    "\t\tmetrics=['accuracy'] )\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "\t\"\"\"The loss function used for all optuna studies\n",
    "\n",
    "\tArgs:\n",
    "\t\ty_true (MatrixLike | ArrayLike): Ground truth (correct) target values\n",
    "\t\ty_pred (MatrixLike | ArrayLike): Estimated target values\n",
    "\n",
    "\tReturns:\n",
    "\t\t_type_: The loss for this prediction (ie, prediction error)\n",
    "\t\"\"\"\n",
    "\n",
    "\tloss = mean_squared_error(y_true, y_pred)\n",
    "\treturn loss\n",
    "\n",
    "class OptunaHyperParamOptimization:\n",
    "\tdef __init__(self, X, y, splits, loss_fnc, random_state):\n",
    "\t\tself.X = X\n",
    "\t\tif len(self.X.shape) == 1: self.X = self.X.reshape(-1,1)\n",
    "\t\tself.y = y\n",
    "\t\tif len(self.y.shape) == 1: self.y = self.y.reshape(-1,1)\n",
    "\t\tself.splits = splits\n",
    "\t\tself.loss_fnc = loss_fnc\n",
    "\t\tself.random_state = random_state\n",
    "\n",
    "\tdef __call__(self, trial:optuna.trial.Trial):\n",
    "\t\treturn None\n",
    "\n",
    "class OptunaHyperParamOptimization_MLP(OptunaHyperParamOptimization):\n",
    "\tdef __call__(self, trial:optuna.trial.Trial):\n",
    "\t\t# average the loss over all cross-validation splits\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor train_idxs, test_idxs in self.splits:\n",
    "\t\t\t# get suggested parameter values from Optuna search space\n",
    "\t\t\tn_hlayers = trial.suggest_int('n_hlayers', 1, 8)\n",
    "\t\t\tn_neurons = trial.suggest_int('n_neurons', 4, 124)\n",
    "\t\t\tact_fnc = trial.suggest_categorical('act_fnc', ['linear','relu','sigmoid','softmax','softplus','tanh'])\n",
    "\t\t\topt_fnc = trial.suggest_categorical('opt_fnc', ['adam', 'sgd'])\n",
    "\t\t\tlearning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "\n",
    "\t\t\t# create sequential NN model\n",
    "\t\t\tmodel = create_model(\n",
    "\t\t\t\tn_hlayers=n_hlayers,\n",
    "\t\t\t\tn_neurons=n_neurons,\n",
    "\t\t\t\tact_fnc=act_fnc,\n",
    "\t\t\t\topt_fnc=opt_fnc,\n",
    "\t\t\t\tlearning_rate=learning_rate,\n",
    "\t\t\t\tinput_shape=self.X.shape[1],\n",
    "\t\t\t\toutput_shape=self.y.shape[1]\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# standardize input and output data (using only the training data to create the scaler)\n",
    "\t\t\tscaler_X = StandardScaler().fit(self.X[train_idxs])\n",
    "\t\t\tscaler_y = StandardScaler().fit(self.y[train_idxs])\n",
    "\t\t\tX_sc = scaler_X.transform(self.X)\n",
    "\t\t\ty_sc = scaler_y.transform(self.y)\n",
    "\n",
    "\t\t\t# fit model to scaled input and output data\n",
    "\t\t\tmodel.fit(X_sc[train_idxs], y_sc[train_idxs])\n",
    "\t\t\t\n",
    "\t\t\t# get predictions\n",
    "\t\t\ty_pred_sc = model.predict(X_sc[test_idxs])\n",
    "\n",
    "\t\t\t# add loss to total\n",
    "\t\t\ttotal_loss += self.loss_fnc(y_sc[test_idxs], y_pred_sc)\n",
    "\n",
    "\t\t# return average cross-validation loss\n",
    "\t\treturn total_loss / len(self.splits)\n",
    "\n",
    "def print_optuna_study_results(study:optuna.Study):\n",
    "\t\"\"\"Prints the best loss and parameters for a given Optuna study\"\"\"\n",
    "\t\n",
    "\tprint()\n",
    "\tprint('*'*100)\n",
    "\tprint(f'  Study: {study.study_name}')\n",
    "\tprint('*'*100)\n",
    "\tprint('  Best Loss: ', study.best_trial.value)\n",
    "\tprint('  Best Params: ')\n",
    "\tfor k,v in study.best_trial.params.items():\n",
    "\t\tprint(f'    {k}: {v}')\n",
    "\tprint('*'*100)\n",
    "\tprint()\n",
    "\n",
    "def perform_hyperparam_optimization(params:dict, dir_results:Path):\n",
    "\tfor key,val in params.items():\n",
    "\t\tstudy = optuna.create_study(\n",
    "\t\t\tstudy_name=key,\n",
    "\t\t\tdirection='minimize', \n",
    "\t\t\tsampler=optuna.samplers.TPESampler(seed=random_state),)\n",
    "\t\tstudy.optimize(\n",
    "\t\t\tfunc = val['objective'],\n",
    "\t\t\tn_trials = val['n_trials'],\n",
    "\t\t\tn_jobs=-1)\n",
    "\t\tif val['save_results']:\n",
    "\t\t\tpickle.dump(\n",
    "\t\t\t\tstudy, \n",
    "\t\t\t\topen(dir_results.joinpath(f\"{val['filename']}.pkl\"), 'wb'),\n",
    "\t\t\t\tprotocol=pickle.HIGHEST_PROTOCOL)\n",
    "\t\telse:\n",
    "\t\t\tprint_optuna_study_results(study)\n",
    "\n",
    "\n",
    "\n",
    "random_state = 13\n",
    "cv_splitter = Custom_CVSplitter(n_splits=3, split_type='group_id', rand_seed=random_state)\n",
    "cv_splits = list(cv_splitter.split(\n",
    "\tX = modeling_data['model_input'], \n",
    "\ty = modeling_data['model_output'], \n",
    "\tcell_ids = modeling_data['cell_id']))\n",
    "\n",
    "# key = study_name\n",
    "hyperparam_opt_params = {\n",
    "\t'Hyperparameters_MLP': {\n",
    "\t\t'objective':\n",
    "\t\t\tOptunaHyperParamOptimization_MLP(\n",
    "\t\t\t\tX=modeling_data['model_input'], \n",
    "\t\t\t\ty=modeling_data['model_output'], \n",
    "\t\t\t\tsplits=cv_splits, \n",
    "\t\t\t\tloss_fnc=loss_function, \n",
    "\t\t\t\trandom_state=random_state\n",
    "\t\t\t),\n",
    "\t\t'n_trials':5000,\n",
    "\t\t'save_results':True,\n",
    "\t\t'filename':'hyperparam_study_mlp'\n",
    "\t},\n",
    "}\n",
    "\n",
    "dir_hyperparam_results = dir_repo_main.joinpath(\"results\", 'hyperparameter_optimization')\n",
    "dir_hyperparam_results.mkdir(exist_ok=True, parents=True)\n",
    "perform_hyperparam_optimization(hyperparam_opt_params, dir_hyperparam_results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envRapidSOH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
