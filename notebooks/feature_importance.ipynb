{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/bnowacki/Documents/Git Repositories/rapid-soh-estimation-from-short-pulses')\n",
    "\n",
    "from rapid_soh_estimation.rapid_soh_estimation.config import *\n",
    "from rapid_soh_estimation.rapid_soh_estimation.common_methods import *\n",
    "\n",
    "\n",
    "cc_data = load_processed_data(data_type='cc')\n",
    "pulse_data = load_processed_data(data_type='slowpulse')\n",
    "\n",
    "\n",
    "# get modeling data (charge pulse only, 20soc only)\n",
    "all_data = deepcopy(pulse_data)\n",
    "idxs = np.where((all_data['pulse_type'] == 'chg') & (all_data['soc'] == 20))\n",
    "for k in all_data.keys():\n",
    "\tall_data[k] = all_data[k][idxs]\n",
    "all_data['voltage_rel'] = np.asarray([v - v[0] for v in all_data['voltage']])\n",
    "modeling_data = create_modeling_data(all_data=all_data, input_feature_keys=['voltage_rel'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-25 12:04:53,062] A new study created in memory with name: Hyperparameters_Lasso\n",
      "[I 2024-09-25 12:04:53,202] Trial 1 finished with value: 1.005039425745166 and parameters: {'alpha': 629343464.2368525}. Best is trial 1 with value: 1.005039425745166.\n",
      "[I 2024-09-25 12:04:53,240] Trial 4 finished with value: 1.005039425745166 and parameters: {'alpha': 409453534.79587364}. Best is trial 1 with value: 1.005039425745166.\n",
      "[I 2024-09-25 12:04:53,323] Trial 0 finished with value: 1.005039425745166 and parameters: {'alpha': 748793361.7200595}. Best is trial 1 with value: 1.005039425745166.\n",
      "[I 2024-09-25 12:04:53,335] Trial 3 finished with value: 1.005039425745166 and parameters: {'alpha': 424257570.36071867}. Best is trial 1 with value: 1.005039425745166.\n",
      "[I 2024-09-25 12:04:53,344] Trial 2 finished with value: 1.005039425745166 and parameters: {'alpha': 796918475.2766232}. Best is trial 1 with value: 1.005039425745166.\n",
      "[I 2024-09-25 12:04:53,345] A new study created in memory with name: Hyperparameters_ElasticNet\n",
      "[I 2024-09-25 12:04:53,454] Trial 0 finished with value: 1.005039425745166 and parameters: {'alpha': 36656308.805184335, 'l1_ratio': 0.17327706869224285}. Best is trial 0 with value: 1.005039425745166.\n",
      "[I 2024-09-25 12:04:53,486] Trial 3 finished with value: 1.005039425745166 and parameters: {'alpha': 907281294.2704102, 'l1_ratio': 0.6693499809122921}. Best is trial 0 with value: 1.005039425745166.\n",
      "[I 2024-09-25 12:04:53,508] Trial 4 finished with value: 1.005039425745166 and parameters: {'alpha': 363315818.33979255, 'l1_ratio': 0.4885977131659839}. Best is trial 0 with value: 1.005039425745166.\n",
      "[I 2024-09-25 12:04:53,531] Trial 2 finished with value: 1.005039425745166 and parameters: {'alpha': 845201518.9234164, 'l1_ratio': 0.3886256821238024}. Best is trial 0 with value: 1.005039425745166.\n",
      "[I 2024-09-25 12:04:53,542] Trial 1 finished with value: 1.005039425745166 and parameters: {'alpha': 410765746.70571387, 'l1_ratio': 0.9139495574515363}. Best is trial 0 with value: 1.005039425745166.\n",
      "[I 2024-09-25 12:04:53,543] A new study created in memory with name: Hyperparameters_RandomForestRegressor\n",
      "[I 2024-09-25 12:04:54,535] Trial 1 finished with value: 1.005034923097651 and parameters: {'n_estimators': 101, 'min_samples_split': 0.5555692520628596, 'min_samples_leaf': 0.2955448722673621, 'max_features': 0.533606404115567, 'max_samples': 0.4284206390120319}. Best is trial 1 with value: 1.005034923097651.\n",
      "[I 2024-09-25 12:04:54,831] Trial 2 finished with value: 1.0052171691669811 and parameters: {'n_estimators': 156, 'min_samples_split': 0.14627207536027065, 'min_samples_leaf': 0.30636937820647675, 'max_features': 0.06134201164465657, 'max_samples': 0.2161326089027764}. Best is trial 1 with value: 1.005034923097651.\n",
      "[I 2024-09-25 12:04:54,908] Trial 3 finished with value: 1.0050665540092134 and parameters: {'n_estimators': 179, 'min_samples_split': 0.24952613392471698, 'min_samples_leaf': 0.7021923866135352, 'max_features': 0.6192829062609586, 'max_samples': 0.48734521815998244}. Best is trial 1 with value: 1.005034923097651.\n",
      "[I 2024-09-25 12:04:55,247] Trial 0 finished with value: 1.0049817579925464 and parameters: {'n_estimators': 393, 'min_samples_split': 0.7287331120357611, 'min_samples_leaf': 0.7597191732551267, 'max_features': 0.06034033479944966, 'max_samples': 0.6110788554089941}. Best is trial 0 with value: 1.0049817579925464.\n",
      "[I 2024-09-25 12:04:55,275] Trial 4 finished with value: 1.005083813655341 and parameters: {'n_estimators': 472, 'min_samples_split': 0.3780969284539084, 'min_samples_leaf': 0.6201781809570786, 'max_features': 0.36846673277900055, 'max_samples': 0.841168355343421}. Best is trial 0 with value: 1.0049817579925464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************************************************************************************************\n",
      "  Study: Hyperparameters_RandomForestRegressor\n",
      "****************************************************************************************************\n",
      "  Best Loss:  1.0045919427665917\n",
      "  Best Params: \n",
      "    n_estimators: 40\n",
      "    min_samples_split: 0.5313796586609383\n",
      "    min_samples_leaf: 0.3508255757943977\n",
      "    max_features: 0.22316970660747334\n",
      "    max_samples: 0.3030590628662242\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "  Study: Hyperparameters_Lasso\n",
      "****************************************************************************************************\n",
      "  Best Loss:  1.005039425745166\n",
      "  Best Params: \n",
      "    alpha: 629343464.2368525\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "  Study: Hyperparameters_RandomForestRegressor\n",
      "****************************************************************************************************\n",
      "  Best Loss:  1.0049817579925464\n",
      "  Best Params: \n",
      "    n_estimators: 393\n",
      "    min_samples_split: 0.7287331120357611\n",
      "    min_samples_leaf: 0.7597191732551267\n",
      "    max_features: 0.06034033479944966\n",
      "    max_samples: 0.6110788554089941\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "  Study: Hyperparameters_ElasticNet\n",
      "****************************************************************************************************\n",
      "  Best Loss:  1.005039425745166\n",
      "  Best Params: \n",
      "    alpha: 36656308.805184335\n",
      "    l1_ratio: 0.17327706869224285\n",
      "****************************************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import optuna\n",
    "\n",
    "# Procedure:\n",
    "#   1. select a model\n",
    "#   2. define a set of hyperparameters\n",
    "#   3. optimize hyperparameters\n",
    "#   4. use optimal hyperparamters / create optimal model\n",
    "#   5. fit model and run CV\n",
    "#   6a. For linear models: average model coeffs over CV and print\n",
    "#\t6b. For RF model: average the feature importance values (Gini importance)\n",
    "\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "\t\"\"\"The loss function used for all optuna studies\n",
    "\n",
    "\tArgs:\n",
    "\t\ty_true (MatrixLike | ArrayLike): Ground truth (correct) target values\n",
    "\t\ty_pred (MatrixLike | ArrayLike): Estimated target values\n",
    "\n",
    "\tReturns:\n",
    "\t\t_type_: The loss for this prediction (ie, prediction error)\n",
    "\t\"\"\"\n",
    "\n",
    "\tloss = mean_squared_error(y_true, y_pred)\n",
    "\treturn loss\n",
    "\n",
    "class OptunaHyperParamOptimization:\n",
    "\tdef __init__(self, X, y, splits, loss_fnc, random_state):\n",
    "\t\tself.X = X\n",
    "\t\tself.y = y\n",
    "\t\tself.splits = splits\n",
    "\t\tself.loss_fnc = loss_fnc\n",
    "\t\tself.random_state = random_state\n",
    "\n",
    "\tdef __call__(self, trial:optuna.trial.Trial):\n",
    "\t\treturn None\n",
    "\n",
    "class OptunaHyperParamOptimization_Ridge(OptunaHyperParamOptimization):\n",
    "\tdef __call__(self, trial:optuna.trial.Trial):\n",
    "\t\t# average the loss over all cross-validation splits\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor train_idxs, test_idxs in self.splits:\n",
    "\t\t\t# get suggest alpha value from Optuna search space\n",
    "\t\t\talpha = trial.suggest_float('alpha', 0.0, 1e9)\n",
    "\t\t\t# create Ridge model\n",
    "\t\t\tmodel = Ridge(\n",
    "\t\t\t\talpha=alpha,\n",
    "\t\t\t\trandom_state=self.random_state\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# standardize input and output data (using only the training data to create the scaler)\n",
    "\t\t\tscaler_X = StandardScaler().fit(self.X[train_idxs])\n",
    "\t\t\tscaler_y = StandardScaler().fit(self.y[train_idxs])\n",
    "\t\t\tX_sc = scaler_X.transform(self.X)\n",
    "\t\t\ty_sc = scaler_y.transform(self.y)\n",
    "\n",
    "\t\t\t# fit model to scaled input and output data\n",
    "\t\t\tmodel.fit(X_sc[train_idxs], y_sc[train_idxs])\n",
    "\t\t\t\n",
    "\t\t\t# get predictions\n",
    "\t\t\ty_pred_sc = model.predict(X_sc[test_idxs])\n",
    "\n",
    "\t\t\t# add loss to total\n",
    "\t\t\ttotal_loss += self.loss_fnc(y_sc[test_idxs], y_pred_sc)\n",
    "\n",
    "\t\t# return average cross-validation loss\n",
    "\t\treturn total_loss / len(self.splits)\n",
    "\n",
    "class OptunaHyperParamOptimization_Lasso(OptunaHyperParamOptimization):\n",
    "\tdef __call__(self, trial:optuna.trial.Trial):\n",
    "\t\t# average the loss over all cross-validation splits\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor train_idxs, test_idxs in self.splits:\n",
    "\t\t\t# get suggested model parameters from Optuna search space\n",
    "\t\t\talpha = trial.suggest_float('alpha', 0, 1e9)\n",
    "\n",
    "\t\t\t# create Lasso model\n",
    "\t\t\tmodel = Lasso(\n",
    "\t\t\t\talpha=alpha,\n",
    "\t\t\t\trandom_state=self.random_state\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# standardize input and output data (using only the training data to create the scaler)\n",
    "\t\t\tscaler_X = StandardScaler().fit(self.X[train_idxs])\n",
    "\t\t\tscaler_y = StandardScaler().fit(self.y[train_idxs])\n",
    "\t\t\tX_sc = scaler_X.transform(self.X)\n",
    "\t\t\ty_sc = scaler_y.transform(self.y)\n",
    "\n",
    "\t\t\t# fit model to scaled input and output data\n",
    "\t\t\tmodel.fit(X_sc[train_idxs], y_sc[train_idxs])\n",
    "\t\t\t\n",
    "\t\t\t# get predictions\n",
    "\t\t\ty_pred_sc = model.predict(X_sc[test_idxs])\n",
    "\n",
    "\t\t\t# add loss to total\n",
    "\t\t\ttotal_loss += self.loss_fnc(y_sc[test_idxs], y_pred_sc)\n",
    "\n",
    "\t\t# return average cross-validation loss\n",
    "\t\treturn total_loss / len(self.splits)\n",
    "\n",
    "class OptunaHyperParamOptimization_ElasticNet(OptunaHyperParamOptimization):\n",
    "\tdef __call__(self, trial:optuna.trial.Trial):\n",
    "\t\t# average the loss over all cross-validation splits\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor train_idxs, test_idxs in self.splits:\n",
    "\t\t\t# get suggested model parameters from Optuna search space\n",
    "\t\t\talpha = trial.suggest_float('alpha', 0.0, 1e9)\n",
    "\t\t\tl1_ratio = trial.suggest_float('l1_ratio', 0.01, 1.0)\n",
    "\n",
    "\t\t\t# create ElasticNet model\n",
    "\t\t\tmodel = ElasticNet(\n",
    "\t\t\t\talpha=alpha, \n",
    "\t\t\t\tl1_ratio=l1_ratio, \n",
    "\t\t\t\trandom_state=self.random_state,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# standardize input and output data (using only the training data to create the scaler)\n",
    "\t\t\tscaler_X = StandardScaler().fit(self.X[train_idxs])\n",
    "\t\t\tscaler_y = StandardScaler().fit(self.y[train_idxs])\n",
    "\t\t\tX_sc = scaler_X.transform(self.X)\n",
    "\t\t\ty_sc = scaler_y.transform(self.y)\n",
    "\n",
    "\t\t\t# fit model to scaled input and output data\n",
    "\t\t\tmodel.fit(X_sc[train_idxs], y_sc[train_idxs])\n",
    "\t\t\t\n",
    "\t\t\t# get predictions\n",
    "\t\t\ty_pred_sc = model.predict(X_sc[test_idxs])\n",
    "\n",
    "\t\t\t# add loss to total\n",
    "\t\t\ttotal_loss += self.loss_fnc(y_sc[test_idxs], y_pred_sc)\n",
    "\n",
    "\t\t# return average cross-validation loss\n",
    "\t\treturn total_loss / len(self.splits)\n",
    "\n",
    "class OptunaHyperParamOptimization_RandomForestRegressor(OptunaHyperParamOptimization):\n",
    "\tdef __call__(self, trial:optuna.trial.Trial):\n",
    "\t\t# average the loss over all cross-validation splits\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor train_idxs, test_idxs in self.splits:\n",
    "\t\t\t# get suggested model parameters from Optuna search space\n",
    "\t\t\tn_estimators = trial.suggest_int('n_estimators', 5, 500)\n",
    "\t\t\tmin_samples_split = trial.suggest_float('min_samples_split', 0.01, 1.0)\n",
    "\t\t\tmin_samples_leaf = trial.suggest_float('min_samples_leaf', 0.01, 1.0)\n",
    "\t\t\tmax_features = trial.suggest_float('max_features', 0.01, 1.0)\n",
    "\t\t\tmax_samples = trial.suggest_float('max_samples', 0.01, 1.0)\n",
    "\n",
    "\t\t\t# create RandomForestRegressor model\n",
    "\t\t\tmodel = RandomForestRegressor(\n",
    "\t\t\t\tn_estimators=n_estimators, \n",
    "\t\t\t\tmin_samples_split=min_samples_split,\n",
    "\t\t\t\tmin_samples_leaf=min_samples_leaf,\n",
    "\t\t\t\tmax_features=max_features,  \n",
    "\t\t\t\tmax_samples=max_samples,\n",
    "\t\t\t\tn_jobs=-1,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# standardize input and output data (using only the training data to create the scaler)\n",
    "\t\t\tscaler_X = StandardScaler().fit(self.X[train_idxs])\n",
    "\t\t\tscaler_y = StandardScaler().fit(self.y[train_idxs])\n",
    "\t\t\tX_sc = scaler_X.transform(self.X)\n",
    "\t\t\ty_sc = scaler_y.transform(self.y)\n",
    "\n",
    "\t\t\t# fit model to scaled input and output data\n",
    "\t\t\tmodel.fit(X_sc[train_idxs], y_sc[train_idxs])\n",
    "\t\t\t\n",
    "\t\t\t# get predictions\n",
    "\t\t\ty_pred_sc = model.predict(X_sc[test_idxs])\n",
    "\n",
    "\t\t\t# add loss to total\n",
    "\t\t\ttotal_loss += self.loss_fnc(y_sc[test_idxs], y_pred_sc)\n",
    "\n",
    "\t\t# return average cross-validation loss\n",
    "\t\treturn total_loss / len(self.splits)\n",
    "\n",
    "\n",
    "\n",
    "random_state = 13\n",
    "\n",
    "cv_splitter = Custom_CVSplitter(n_splits=3, split_type='group_id', rand_seed=random_state)\n",
    "cv_splits = list(cv_splitter.split(\n",
    "\tX = modeling_data['model_input'], \n",
    "\ty = modeling_data['model_output'], \n",
    "\tcell_ids = modeling_data['cell_id']))\n",
    "\n",
    "dir_hyperparam_results = dir_repo_main.joinpath(\"results\", 'hyperparameter_optimization')\n",
    "dir_hyperparam_results.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# key = study_name\n",
    "hyperparam_opt_params = {\n",
    "\t'Hyperparameters_Ridge': {\n",
    "\t\t'objective':\n",
    "\t\t\tOptunaHyperParamOptimization_Ridge(\n",
    "\t\t\t\tX=modeling_data['model_input'], \n",
    "\t\t\t\ty=modeling_data['model_output'], \n",
    "\t\t\t\tsplits=cv_splits, \n",
    "\t\t\t\tloss_fnc=loss_function, \n",
    "\t\t\t\trandom_state=random_state\n",
    "\t\t\t),\n",
    "\t\t'n_trials':100,\n",
    "\t\t'save_results':True,\n",
    "\t\t'filename':'hyperparam_study_ridge'\n",
    "\t},\n",
    "\t'Hyperparameters_Lasso': {\n",
    "\t\t'objective':\n",
    "\t\t\tOptunaHyperParamOptimization_Lasso(\n",
    "\t\t\t\tX=modeling_data['model_input'], \n",
    "\t\t\t\ty=modeling_data['model_output'], \n",
    "\t\t\t\tsplits=cv_splits, \n",
    "\t\t\t\tloss_fnc=loss_function, \n",
    "\t\t\t\trandom_state=random_state\n",
    "\t\t\t),\n",
    "\t\t'n_trials':5,\n",
    "\t\t'save_results':True,\n",
    "\t\t'filename':'hyperparam_study_lasso'\n",
    "\t},\n",
    "\t'Hyperparameters_ElasticNet': {\n",
    "\t\t'objective':\n",
    "\t\t\tOptunaHyperParamOptimization_ElasticNet(\n",
    "\t\t\t\tX=modeling_data['model_input'], \n",
    "\t\t\t\ty=modeling_data['model_output'], \n",
    "\t\t\t\tsplits=cv_splits, \n",
    "\t\t\t\tloss_fnc=loss_function, \n",
    "\t\t\t\trandom_state=random_state\n",
    "\t\t\t),\n",
    "\t\t'n_trials':5,\n",
    "\t\t'save_results':True,\n",
    "\t\t'filename':'hyperparam_study_elasticnet'\n",
    "\t},\n",
    "\t'Hyperparameters_RandomForestRegressor': {\n",
    "\t\t'objective':\n",
    "\t\t\tOptunaHyperParamOptimization_RandomForestRegressor(\n",
    "\t\t\t\tX=modeling_data['model_input'], \n",
    "\t\t\t\ty=modeling_data['model_output'], \n",
    "\t\t\t\tsplits=cv_splits, \n",
    "\t\t\t\tloss_fnc=loss_function, \n",
    "\t\t\t\trandom_state=random_state\n",
    "\t\t\t),\n",
    "\t\t'n_trials':5,\n",
    "\t\t'save_results':True,\n",
    "\t\t'filename':'hyperparam_study_randomforest'\n",
    "\t},\n",
    "}\n",
    "\n",
    "\n",
    "def print_optuna_study_results(study:optuna.Study):\n",
    "\t\"\"\"Prints the best loss and parameters for a given Optuna study\"\"\"\n",
    "\t\n",
    "\tprint()\n",
    "\tprint('*'*100)\n",
    "\tprint(f'  Study: {study.study_name}')\n",
    "\tprint('*'*100)\n",
    "\tprint('  Best Loss: ', study.best_trial.value)\n",
    "\tprint('  Best Params: ')\n",
    "\tfor k,v in study.best_trial.params.items():\n",
    "\t\tprint(f'    {k}: {v}')\n",
    "\tprint('*'*100)\n",
    "\tprint()\n",
    "\n",
    "def perform_hyperparam_optimization(params:dict):\n",
    "\tfor key,val in params.items():\n",
    "\t\tstudy = optuna.create_study(\n",
    "\t\t\tstudy_name=key,\n",
    "\t\t\tdirection='minimize', \n",
    "\t\t\tsampler=optuna.samplers.TPESampler(seed=random_state),)\n",
    "\t\tstudy.optimize(\n",
    "\t\t\tfunc = val['objective'],\n",
    "\t\t\tn_trials = val['n_trials'],\n",
    "\t\t\tn_jobs=-1)\n",
    "\t\tif val['save_results']:\n",
    "\t\t\tpickle.dump(\n",
    "\t\t\t\tstudy, \n",
    "\t\t\t\topen(dir_hyperparam_results.joinpath(f\"{val['filename']}.pkl\"), 'wb'),\n",
    "\t\t\t\tprotocol=pickle.HIGHEST_PROTOCOL)\n",
    "\t\telse:\n",
    "\t\t\tprint_optuna_study_results(study)\n",
    "\n",
    "\n",
    "\n",
    "perform_hyperparam_optimization(hyperparam_opt_params)\n",
    "\n",
    "# print best hyperparameters for all studies in dir_hyperparam_results\n",
    "for file_res in dir_hyperparam_results.glob('*.pkl'):\n",
    "\tstudy = pickle.load(open(file_res, 'rb'))\n",
    "\tprint_optuna_study_results(study)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envRapidSOH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
