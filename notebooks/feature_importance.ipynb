{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/bnowacki/Documents/Git Repositories/rapid-soh-estimation-from-short-pulses')\n",
    "\n",
    "from rapid_soh_estimation.rapid_soh_estimation.config import *\n",
    "from rapid_soh_estimation.rapid_soh_estimation.common_methods import *\n",
    "\n",
    "\n",
    "cc_data = load_processed_data(data_type='cc')\n",
    "pulse_data = load_processed_data(data_type='slowpulse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "\t\"\"\"The loss function used for all optuna studies\n",
    "\n",
    "\tArgs:\n",
    "\t\ty_true (MatrixLike | ArrayLike): Ground truth (correct) target values\n",
    "\t\ty_pred (MatrixLike | ArrayLike): Estimated target values\n",
    "\n",
    "\tReturns:\n",
    "\t\t_type_: The loss for this prediction (ie, prediction error)\n",
    "\t\"\"\"\n",
    "\n",
    "\tloss = mean_squared_error(y_true, y_pred)\n",
    "\treturn loss\n",
    "\n",
    "class OptunaHyperParamOptimization:\n",
    "\tdef __init__(self, X, y, splits, loss_fnc, random_state):\n",
    "\t\tself.X = X\n",
    "\t\tif len(self.X.shape) == 1: self.X = self.X.reshape(-1,1)\n",
    "\t\tself.y = y\n",
    "\t\tif len(self.y.shape) == 1: self.y = self.y.reshape(-1,1)\n",
    "\t\tself.splits = splits\n",
    "\t\tself.loss_fnc = loss_fnc\n",
    "\t\tself.random_state = random_state\n",
    "\n",
    "\tdef __call__(self, trial:optuna.trial.Trial):\n",
    "\t\treturn None\n",
    "\n",
    "class OptunaHyperParamOptimization_Ridge(OptunaHyperParamOptimization):\n",
    "\tdef __call__(self, trial:optuna.trial.Trial):\n",
    "\t\t# average the loss over all cross-validation splits\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor train_idxs, test_idxs in self.splits:\n",
    "\t\t\t# get suggest alpha value from Optuna search space\n",
    "\t\t\talpha = trial.suggest_float('alpha', 0.0, 1e4)\n",
    "\t\t\t# create Ridge model\n",
    "\t\t\tmodel = Ridge(\n",
    "\t\t\t\talpha=alpha,\n",
    "\t\t\t\trandom_state=self.random_state\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# standardize input and output data (using only the training data to create the scaler)\n",
    "\t\t\tscaler_X = StandardScaler().fit(self.X[train_idxs])\n",
    "\t\t\tscaler_y = StandardScaler().fit(self.y[train_idxs])\n",
    "\t\t\tX_sc = scaler_X.transform(self.X)\n",
    "\t\t\ty_sc = scaler_y.transform(self.y)\n",
    "\n",
    "\t\t\t# fit model to scaled input and output data\n",
    "\t\t\tmodel.fit(X_sc[train_idxs], y_sc[train_idxs])\n",
    "\t\t\t\n",
    "\t\t\t# get predictions\n",
    "\t\t\ty_pred_sc = model.predict(X_sc[test_idxs])\n",
    "\n",
    "\t\t\t# add loss to total\n",
    "\t\t\ttotal_loss += self.loss_fnc(y_sc[test_idxs], y_pred_sc)\n",
    "\n",
    "\t\t# return average cross-validation loss\n",
    "\t\treturn total_loss / len(self.splits)\n",
    "\n",
    "class OptunaHyperParamOptimization_Lasso(OptunaHyperParamOptimization):\n",
    "\tdef __call__(self, trial:optuna.trial.Trial):\n",
    "\t\t# average the loss over all cross-validation splits\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor train_idxs, test_idxs in self.splits:\n",
    "\t\t\t# get suggested model parameters from Optuna search space\n",
    "\t\t\talpha = trial.suggest_float('alpha', 0, 1.0)\n",
    "\n",
    "\t\t\t# create Lasso model\n",
    "\t\t\tmodel = Lasso(\n",
    "\t\t\t\talpha=alpha,\n",
    "\t\t\t\trandom_state=self.random_state\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# standardize input and output data (using only the training data to create the scaler)\n",
    "\t\t\tscaler_X = StandardScaler().fit(self.X[train_idxs])\n",
    "\t\t\tscaler_y = StandardScaler().fit(self.y[train_idxs])\n",
    "\t\t\tX_sc = scaler_X.transform(self.X)\n",
    "\t\t\ty_sc = scaler_y.transform(self.y)\n",
    "\n",
    "\t\t\t# fit model to scaled input and output data\n",
    "\t\t\tmodel.fit(X_sc[train_idxs], y_sc[train_idxs])\n",
    "\t\t\t\n",
    "\t\t\t# get predictions\n",
    "\t\t\ty_pred_sc = model.predict(X_sc[test_idxs])\n",
    "\n",
    "\t\t\t# add loss to total\n",
    "\t\t\ttotal_loss += self.loss_fnc(y_sc[test_idxs], y_pred_sc)\n",
    "\n",
    "\t\t# return average cross-validation loss\n",
    "\t\treturn total_loss / len(self.splits)\n",
    "\n",
    "class OptunaHyperParamOptimization_ElasticNet(OptunaHyperParamOptimization):\n",
    "\tdef __call__(self, trial:optuna.trial.Trial):\n",
    "\t\t# average the loss over all cross-validation splits\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor train_idxs, test_idxs in self.splits:\n",
    "\t\t\t# get suggested model parameters from Optuna search space\n",
    "\t\t\talpha = trial.suggest_float('alpha', 0.0, 1e3)\n",
    "\t\t\tl1_ratio = trial.suggest_float('l1_ratio', 0.01, 1.0)\n",
    "\n",
    "\t\t\t# create ElasticNet model\n",
    "\t\t\tmodel = ElasticNet(\n",
    "\t\t\t\talpha=alpha, \n",
    "\t\t\t\tl1_ratio=l1_ratio, \n",
    "\t\t\t\trandom_state=self.random_state,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# standardize input and output data (using only the training data to create the scaler)\n",
    "\t\t\tscaler_X = StandardScaler().fit(self.X[train_idxs])\n",
    "\t\t\tscaler_y = StandardScaler().fit(self.y[train_idxs])\n",
    "\t\t\tX_sc = scaler_X.transform(self.X)\n",
    "\t\t\ty_sc = scaler_y.transform(self.y)\n",
    "\n",
    "\t\t\t# fit model to scaled input and output data\n",
    "\t\t\tmodel.fit(X_sc[train_idxs], y_sc[train_idxs])\n",
    "\t\t\t\n",
    "\t\t\t# get predictions\n",
    "\t\t\ty_pred_sc = model.predict(X_sc[test_idxs])\n",
    "\n",
    "\t\t\t# add loss to total\n",
    "\t\t\ttotal_loss += self.loss_fnc(y_sc[test_idxs], y_pred_sc)\n",
    "\n",
    "\t\t# return average cross-validation loss\n",
    "\t\treturn total_loss / len(self.splits)\n",
    "\n",
    "class OptunaHyperParamOptimization_RandomForestRegressor(OptunaHyperParamOptimization):\n",
    "\tdef __call__(self, trial:optuna.trial.Trial):\n",
    "\t\t# average the loss over all cross-validation splits\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor train_idxs, test_idxs in self.splits:\n",
    "\t\t\t# get suggested model parameters from Optuna search space\n",
    "\t\t\tn_estimators = trial.suggest_int('n_estimators', 5, 500)\n",
    "\t\t\tmin_samples_split = trial.suggest_float('min_samples_split', 0.01, 1.0)\n",
    "\t\t\tmin_samples_leaf = trial.suggest_float('min_samples_leaf', 0.01, 1.0)\n",
    "\t\t\tmax_features = trial.suggest_float('max_features', 0.01, 1.0)\n",
    "\t\t\tmax_samples = trial.suggest_float('max_samples', 0.01, 1.0)\n",
    "\n",
    "\t\t\t# create RandomForestRegressor model\n",
    "\t\t\tmodel = RandomForestRegressor(\n",
    "\t\t\t\tn_estimators=n_estimators, \n",
    "\t\t\t\tmin_samples_split=min_samples_split,\n",
    "\t\t\t\tmin_samples_leaf=min_samples_leaf,\n",
    "\t\t\t\tmax_features=max_features,  \n",
    "\t\t\t\tmax_samples=max_samples,\n",
    "\t\t\t\tn_jobs=-1,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# standardize input and output data (using only the training data to create the scaler)\n",
    "\t\t\tscaler_X = StandardScaler().fit(self.X[train_idxs])\n",
    "\t\t\tscaler_y = StandardScaler().fit(self.y[train_idxs])\n",
    "\t\t\tX_sc = scaler_X.transform(self.X)\n",
    "\t\t\ty_sc = scaler_y.transform(self.y)\n",
    "\n",
    "\t\t\t# fit model to scaled input and output data\n",
    "\t\t\tmodel.fit(X_sc[train_idxs], y_sc[train_idxs])\n",
    "\t\t\t\n",
    "\t\t\t# get predictions\n",
    "\t\t\ty_pred_sc = model.predict(X_sc[test_idxs])\n",
    "\n",
    "\t\t\t# add loss to total\n",
    "\t\t\ttotal_loss += self.loss_fnc(y_sc[test_idxs], y_pred_sc)\n",
    "\n",
    "\t\t# return average cross-validation loss\n",
    "\t\treturn total_loss / len(self.splits)\n",
    "\n",
    "def print_optuna_study_results(study:optuna.Study):\n",
    "\t\"\"\"Prints the best loss and parameters for a given Optuna study\"\"\"\n",
    "\t\n",
    "\tprint()\n",
    "\tprint('*'*100)\n",
    "\tprint(f'  Study: {study.study_name}')\n",
    "\tprint('*'*100)\n",
    "\tprint('  Best Loss: ', study.best_trial.value)\n",
    "\tprint('  Best Params: ')\n",
    "\tfor k,v in study.best_trial.params.items():\n",
    "\t\tprint(f'    {k}: {v}')\n",
    "\tprint('*'*100)\n",
    "\tprint()\n",
    "\n",
    "def perform_hyperparam_optimization(params:dict):\n",
    "\tfor key,val in params.items():\n",
    "\t\tstudy = optuna.create_study(\n",
    "\t\t\tstudy_name=key,\n",
    "\t\t\tdirection='minimize', \n",
    "\t\t\tsampler=optuna.samplers.TPESampler(seed=random_state),)\n",
    "\t\tstudy.optimize(\n",
    "\t\t\tfunc = val['objective'],\n",
    "\t\t\tn_trials = val['n_trials'],\n",
    "\t\t\tn_jobs=-1)\n",
    "\t\tif val['save_results']:\n",
    "\t\t\tpickle.dump(\n",
    "\t\t\t\tstudy, \n",
    "\t\t\t\topen(dir_hyperparam_results.joinpath(f\"{val['filename']}.pkl\"), 'wb'),\n",
    "\t\t\t\tprotocol=pickle.HIGHEST_PROTOCOL)\n",
    "\t\telse:\n",
    "\t\t\tprint_optuna_study_results(study)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "# use only q_dchg target (drop resistances)\n",
    "# get modeling data (charge pulse only)\n",
    "all_data = deepcopy(pulse_data)\n",
    "idxs = np.where((all_data['pulse_type'] == 'chg'))\n",
    "for k in all_data.keys():\n",
    "\tall_data[k] = all_data[k][idxs]\n",
    "all_data['voltage_rel'] = np.asarray([v - v[0] for v in all_data['voltage']])\n",
    "modeling_data = create_modeling_data(all_data=all_data, input_feature_keys=['voltage_rel'])\n",
    "modeling_data['model_output'] = modeling_data['model_output'][:,0]\n",
    "\n",
    "cv_splitter = Custom_CVSplitter(n_splits=3, split_type='group_id', rand_seed=random_state)\n",
    "cv_splits = list(cv_splitter.split(\n",
    "\tX = modeling_data['model_input'], \n",
    "\ty = modeling_data['model_output'], \n",
    "\tcell_ids = modeling_data['cell_id']))\n",
    "\n",
    "dir_hyperparam_results = dir_repo_main.joinpath(\"results\", 'hyperparameter_optimization')\n",
    "dir_hyperparam_results.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# key = study_name\n",
    "hyperparam_opt_params = {\n",
    "\t'Hyperparameters_Ridge': {\n",
    "\t\t'objective':\n",
    "\t\t\tOptunaHyperParamOptimization_Ridge(\n",
    "\t\t\t\tX=modeling_data['model_input'], \n",
    "\t\t\t\ty=modeling_data['model_output'], \n",
    "\t\t\t\tsplits=cv_splits, \n",
    "\t\t\t\tloss_fnc=loss_function, \n",
    "\t\t\t\trandom_state=random_state\n",
    "\t\t\t),\n",
    "\t\t'n_trials':1000,\n",
    "\t\t'save_results':True,\n",
    "\t\t'filename':'hyperparam_study_ridge_onlySOH_byCell'\n",
    "\t},\n",
    "\t'Hyperparameters_Lasso': {\n",
    "\t\t'objective':\n",
    "\t\t\tOptunaHyperParamOptimization_Lasso(\n",
    "\t\t\t\tX=modeling_data['model_input'], \n",
    "\t\t\t\ty=modeling_data['model_output'], \n",
    "\t\t\t\tsplits=cv_splits, \n",
    "\t\t\t\tloss_fnc=loss_function, \n",
    "\t\t\t\trandom_state=random_state\n",
    "\t\t\t),\n",
    "\t\t'n_trials':1000,\n",
    "\t\t'save_results':True,\n",
    "\t\t'filename':'hyperparam_study_lasso_onlySOH'\n",
    "\t},\n",
    "\t'Hyperparameters_ElasticNet': {\n",
    "\t\t'objective':\n",
    "\t\t\tOptunaHyperParamOptimization_ElasticNet(\n",
    "\t\t\t\tX=modeling_data['model_input'], \n",
    "\t\t\t\ty=modeling_data['model_output'], \n",
    "\t\t\t\tsplits=cv_splits, \n",
    "\t\t\t\tloss_fnc=loss_function, \n",
    "\t\t\t\trandom_state=random_state\n",
    "\t\t\t),\n",
    "\t\t'n_trials':1000,\n",
    "\t\t'save_results':True,\n",
    "\t\t'filename':'hyperparam_study_elasticnet_onlySOH'\n",
    "\t},\n",
    "\t'Hyperparameters_RandomForestRegressor': {\n",
    "\t\t'objective':\n",
    "\t\t\tOptunaHyperParamOptimization_RandomForestRegressor(\n",
    "\t\t\t\tX=modeling_data['model_input'], \n",
    "\t\t\t\ty=modeling_data['model_output'], \n",
    "\t\t\t\tsplits=cv_splits, \n",
    "\t\t\t\tloss_fnc=loss_function, \n",
    "\t\t\t\trandom_state=random_state\n",
    "\t\t\t),\n",
    "\t\t'n_trials':2000,\n",
    "\t\t'save_results':True,\n",
    "\t\t'filename':'hyperparam_study_randomforest_onlySOH'\n",
    "\t},\n",
    "\n",
    "}\n",
    "\n",
    "# perform_hyperparam_optimization(hyperparam_opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_hyperparam_results = dir_repo_main.joinpath(\"results\", 'hyperparameter_optimization')\n",
    "dir_hyperparam_results.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# print best hyperparameters for all studies in dir_hyperparam_results\n",
    "for file_res in dir_hyperparam_results.glob('*_onlySOH.pkl'):\n",
    "\tstudy = pickle.load(open(file_res, 'rb'))\n",
    "\tprint_optuna_study_results(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(dir_hyperparam_results:Path, dir_save:Path=None):\n",
    "\t\"\"\"Plots the feature importance of the pulse signal using linear models and a random forest regressor.\n",
    "\n",
    "\tArgs:\n",
    "\t\tdir_hyperparam_results (Path): Path object to folder containing hyperparameter results\n",
    "\t\tdir_save (Path, optional): If provided, the plot will be saved in this folder. Defaults to None.\n",
    "\t\"\"\"\n",
    "\n",
    "\tplot_params = {\n",
    "\t\t'ridge': {'legend_key':'Ridge', 'color':'C0',},\n",
    "\t\t'lasso': {'legend_key':'Lasso', 'color':'C1',},\n",
    "\t\t'elasticnet': {'legend_key':'ElasticNet', 'color':'C2',},\n",
    "\t\t'randomforest': {'legend_key':'RandomForest', 'color':'C4',},\n",
    "\t}\n",
    "\n",
    "\t#region: prepare data for plotting\n",
    "\t#region: create modeling dataset (charge pulse, all SOCs)\n",
    "\t# load pulse and cc data\n",
    "\tcc_data = load_processed_data(data_type='cc')\n",
    "\tpulse_data = load_processed_data(data_type='slowpulse')\n",
    "\t# create modeling data\n",
    "\tall_data = deepcopy(pulse_data)\n",
    "\tidxs = np.where((all_data['pulse_type'] == 'chg'))\n",
    "\tfor k in all_data.keys():\n",
    "\t\tall_data[k] = all_data[k][idxs]\n",
    "\tall_data['voltage_rel'] = np.asarray([v - v[0] for v in all_data['voltage']])\n",
    "\tmodeling_data = create_modeling_data(all_data=all_data, input_feature_keys=['voltage_rel'])\n",
    "\tX = modeling_data['model_input']\n",
    "\ty = modeling_data['model_output'][:,0].reshape(-1,1)\n",
    "\t#endregion\n",
    "\n",
    "\t#region: get CV splits\n",
    "\tcv_splitter = Custom_CVSplitter(n_splits=3, split_type='group_id', rand_seed=random_state)\n",
    "\tcv_splits = list(cv_splitter.split(\n",
    "\t\tX = modeling_data['model_input'], \n",
    "\t\ty = modeling_data['model_output'][:,0], \n",
    "\t\tcell_ids = modeling_data['cell_id']))\n",
    "\t#endregion\n",
    "\n",
    "\t# use a fixed split for plotting\n",
    "\ttrain_idxs, test_idxs = cv_splits[0]\n",
    "\n",
    "\t# standardize data using only training samples\n",
    "\tscaler_X = StandardScaler().fit(X[train_idxs])\n",
    "\tscaler_y = StandardScaler().fit(y[train_idxs])\n",
    "\tX_sc = scaler_X.transform(X)\n",
    "\ty_sc = scaler_y.transform(y)\n",
    "\t#endregion\n",
    "\n",
    "\tfig = plt.figure(figsize=(6.25,3.75))\n",
    "\tgs = GridSpec(nrows=2, ncols=2, height_ratios=[3,2], width_ratios=[1,1])\n",
    "\taxes = [\n",
    "\t\tfig.add_subplot(gs[0, 0]), fig.add_subplot(gs[0, 1]),\n",
    "\t\tfig.add_subplot(gs[1, 0]), fig.add_subplot(gs[1, 1])\n",
    "\t]\n",
    "\t# axes[2].sharex(axes[0])\n",
    "\t# axes[3].sharex(axes[1])\n",
    "\taxes[0].set_xticklabels([])\n",
    "\taxes[1].set_xticklabels([])\n",
    "\n",
    "\tfor model_type in plot_params.keys():\n",
    "\t\tfile_study = list(dir_hyperparam_results.glob(f'*{model_type}_onlySOH.pkl*'))[0]\n",
    "\t\tstudy = pickle.load(open(file_study, 'rb'))\n",
    "\t\t# create optimal model from saved parameters values\n",
    "\t\tmodel = None\n",
    "\t\tif model_type == 'ridge': model = Ridge(**study.best_trial.params)\n",
    "\t\tif model_type == 'lasso': model = Lasso(**study.best_trial.params)\n",
    "\t\tif model_type == 'elasticnet': model = ElasticNet(**study.best_trial.params)\n",
    "\t\tif model_type == 'randomforest': model = RandomForestRegressor(**study.best_trial.params)\n",
    "\t\t# fit model to training data\n",
    "\t\tmodel.fit(X_sc[train_idxs], y_sc[train_idxs])\n",
    "\n",
    "\t\tx_vals = None\n",
    "\t\tif model_type == 'randomforest':\n",
    "\t\t\tx_vals = model.feature_importances_\n",
    "\t\t\taxes[1].plot(\n",
    "\t\t\t\tx_vals*100, '-', linewidth=2,\n",
    "\t\t\t\tcolor=plot_params[model_type]['color'],\n",
    "\t\t\t\tlabel=plot_params[model_type]['legend_key'])\n",
    "\t\telse:\n",
    "\t\t\tx_vals = model.coef_\n",
    "\t\t\tif len(x_vals.shape) == 2: x_vals = np.ravel(x_vals)\n",
    "\t\t\taxes[0].plot(\n",
    "\t\t\t\tx_vals, '-', linewidth=2,\n",
    "\t\t\t\tcolor=plot_params[model_type]['color'],\n",
    "\t\t\t\tlabel=plot_params[model_type]['legend_key'])\n",
    "\n",
    "\t\tif model_type == 'ridge':\n",
    "\t\t\t# region: overlay feature importance on pulse profile\n",
    "\t\t\tpulse_v = X[train_idxs][0,:]*1000\n",
    "\t\t\taxes[2].plot(pulse_v, 'k', linewidth=2)\n",
    "\t\t\tybounds = [np.min(pulse_v), np.max(pulse_v)]\n",
    "\t\t\tybounds[0] -= (ybounds[1]-ybounds[0])*0.05\n",
    "\t\t\tybounds[1] += (ybounds[1]-ybounds[0])*0.05\n",
    "\n",
    "\t\t\tnorm_coeff = abs(x_vals)\n",
    "\t\t\tnorm_coeff = (norm_coeff - np.min(norm_coeff)) / (np.max(norm_coeff) - np.min(norm_coeff))\n",
    "\t\t\tfor i in np.arange(0,100,1):\n",
    "\t\t\t\taxes[2].fill_betweenx(\n",
    "\t\t\t\t\tybounds, i-0.5, i+0.5, \n",
    "\t\t\t\t\tcolor=plot_params[model_type]['color'], alpha=norm_coeff[i])\n",
    "\t\t\taxes[2].set_xlim([-5,104])\n",
    "\t\t\taxes[2].set_ylim(ybounds)\n",
    "\t\t\t#endregion\n",
    "\t\telif model_type == 'randomforest':\n",
    "\t\t\t# region: overlay feature importance on pulse profile\n",
    "\t\t\tpulse_v = X[train_idxs][0,:]*1000\n",
    "\t\t\taxes[3].plot(pulse_v, 'k', linewidth=2)\n",
    "\t\t\tybounds = [np.min(pulse_v), np.max(pulse_v)]\n",
    "\t\t\tybounds[0] -= (ybounds[1]-ybounds[0])*0.05\n",
    "\t\t\tybounds[1] += (ybounds[1]-ybounds[0])*0.05\n",
    "\n",
    "\t\t\tnorm_coeff = abs(x_vals)\n",
    "\t\t\tnorm_coeff = (norm_coeff - np.min(norm_coeff)) / (np.max(norm_coeff) - np.min(norm_coeff))\n",
    "\t\t\tfor i in np.arange(0,100,1):\n",
    "\t\t\t\taxes[3].fill_betweenx(\n",
    "\t\t\t\t\tybounds, i-0.5, i+0.5, \n",
    "\t\t\t\t\tcolor=plot_params[model_type]['color'], alpha=norm_coeff[i])\n",
    "\t\t\taxes[3].set_xlim([-5,104])\n",
    "\t\t\taxes[3].set_ylim(ybounds)\n",
    "\t\t\t#endregion\n",
    "\t#region: set labels\n",
    "\taxes[0].plot([0,99],[0,0], '--', color='k')\n",
    "\taxes[0].set_ylabel(\"Model Coefficients [-]\")\n",
    "\taxes[0].set_title('Linear Models')\n",
    "\taxes[0].legend(fontsize=8, loc='lower right')\n",
    "\n",
    "\taxes[1].set_ylabel(\"Feature Importance [%]\")\n",
    "\taxes[1].set_title('Random Forest')\n",
    "\n",
    "\taxes[2].set_xlabel(\"Time [s]\")\n",
    "\taxes[2].set_ylabel(\"Rel. Voltage [mV]\")\n",
    "\taxes[3].set_xlabel(\"Time [s]\")\n",
    "\taxes[3].set_ylabel(\"Rel. Voltage [mV]\")\n",
    "\tfig.align_ylabels()\n",
    "\t#endregion\n",
    "\t\n",
    "\tfig.tight_layout(h_pad=0.5, w_pad=1.2)\n",
    "\tif dir_save is not None:\n",
    "\t\tfilename = dir_save.joinpath(\"FeatureImportance_Pulse.pdf\")\n",
    "\t\tfig.savefig(filename, dpi=300)\n",
    "\tplt.show()\n",
    "\n",
    "dir_save = dir_figures.joinpath(\"raw\", \"Feature Importance\")\n",
    "dir_save.mkdir(exist_ok=True, parents=True)\n",
    "plot_feature_importance(\n",
    "\tdir_hyperparam_results = dir_repo_main.joinpath(\"results\", 'hyperparameter_optimization'),\n",
    "\tdir_save = dir_save\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance(dir_hyperparam_results:Path, dir_save:Path=None):\n",
    "\t\"\"\"Plots the baseline model perforamnce for linear models, random forest regressor, and MLP\n",
    "\n",
    "\tArgs:\n",
    "\t\tdir_hyperparam_results (Path): Path object to folder containing hyperparameter results\n",
    "\t\tdir_save (Path, optional): If provided, the plot will be saved in this folder. Defaults to None.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef _evaluate_model(model, X, y, cv_splits) -> dict:\n",
    "\t\t\"\"\"Evaluates the train and test accuracy of a given model\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tmodel (-): A model to be tested. Must have .fit() and .predict() functions\n",
    "\t\t\tX (_type_): Training data\n",
    "\t\t\ty (_type_): Test data\n",
    "\t\t\tcv_splits (_type_): A list of indices for train/test splits at each CV fold\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tdict: Returns the average and std of the train and test error. Follows the following structure: {'train':(avg, std), 'test':(avg, std)}\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# average the loss over all cross-validation splits\n",
    "\t\ttest_err = []\n",
    "\t\ttrain_err = []\n",
    "\t\tfor train_idxs, test_idxs in cv_splits:\n",
    "\t\t\t# standardize input and output data (using only the training data to create the scaler)\n",
    "\t\t\tscaler_X = StandardScaler().fit(X[train_idxs])\n",
    "\t\t\tscaler_y = StandardScaler().fit(y[train_idxs])\n",
    "\t\t\tX_sc = scaler_X.transform(X)\n",
    "\t\t\ty_sc = scaler_y.transform(y)\n",
    "\t\t\t# fit model to scaled input and output data\n",
    "\t\t\tmodel.fit(X_sc[train_idxs], y_sc[train_idxs])\n",
    "\t\t\t\n",
    "\t\t\t# get test and train predictions\n",
    "\t\t\tyhat_train = model.predict(X_sc[train_idxs])\n",
    "\t\t\tyhat_test = model.predict(X_sc[test_idxs])\n",
    "\t\t\ttrain_err.append( mean_absolute_percentage_error(y_sc[train_idxs], yhat_train))\n",
    "\t\t\ttest_err.append( mean_absolute_percentage_error(y_sc[test_idxs], yhat_test))\n",
    "\n",
    "\t\t# return average and std of mape\n",
    "\t\treturn {\n",
    "\t\t\t'test':(np.average(np.asarray(test_err)), np.std(np.asarray(test_err))),\n",
    "\t\t\t'train':(np.average(np.asarray(train_err)), np.std(np.asarray(train_err)))\n",
    "\t\t}\n",
    "\t\n",
    "\tdef _create_mlp_model(n_hlayers:int, n_neurons:int, act_fnc:str, opt_fnc:str, learning_rate:float, input_shape=(100,), output_shape=(7,)) -> keras.models.Sequential:\n",
    "\t\t\"\"\"Builds a Keras neural network model (MLP) using the specified parameters. The model is optimized for accuracy. Make sure model outputs (if multiple target) are normalized, otherwise optimization will be biased towards one target variable.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tn_hlayers (int): Number of fully-connected hidden layers\n",
    "\t\t\tn_neurons (int): Number of neurons per hidden layer\n",
    "\t\t\tact_fnc (str): Activation function to use (\\'tanh\\', \\'relu\\', etc)\n",
    "\t\t\topt_fnc (str): {\\'sgd\\', \\'adam\\'} Optimizer function to use \n",
    "\t\t\tlearning_rate (float): Learning rate\n",
    "\t\t\tinput_shape (int, optional): Input shape of model. Defaults to (100,).\n",
    "\t\t\toutput_shape (int, optional): Output shape of model. Default to (7,).\n",
    "\t\tRaises:\n",
    "\t\t\tValueError: _description_\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tkeras.models.Sequential: compiled Keras model\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# add input layer to Sequential model\n",
    "\t\tmodel = keras.models.Sequential()\n",
    "\t\tmodel.add( keras.Input(shape=input_shape) )\n",
    "\n",
    "\t\t# add hidden layers\n",
    "\t\tfor i in range(n_hlayers):\n",
    "\t\t\tmodel.add( keras.layers.Dense(units=n_neurons, activation=act_fnc) )\n",
    "\t\t\t\n",
    "\t\t# add output layer\n",
    "\t\tmodel.add( keras.layers.Dense(output_shape[0]) )\n",
    "\n",
    "\t\t# compile model with chosen metrics\n",
    "\t\topt = None\n",
    "\t\tif opt_fnc == 'adam':\n",
    "\t\t\topt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\t\telif opt_fnc == 'sgd':\n",
    "\t\t\topt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(\"opt_func must be either \\'adam\\' or \\'sgd\\'\")\n",
    "\n",
    "\t\tmodel.compile(\n",
    "\t\t\toptimizer=opt,\n",
    "\t\t\tloss=keras.losses.mean_squared_error,      \n",
    "\t\t\t# make sure to normalize all outputs, otherwise DCIR values will drastically skew MSE reading compared to error of predicted SOH\n",
    "\t\t\tmetrics=['accuracy'] )\n",
    "\t\treturn model\n",
    "\n",
    "\tdir_hyperparam_results = dir_repo_main.joinpath(\"results\", 'hyperparameter_optimization')\n",
    "\tplot_params = {\n",
    "\t\t'ridge': {'legend_key':'Ridge', 'color':'C0',},\n",
    "\t\t'lasso': {'legend_key':'Lasso', 'color':'C1',},\n",
    "\t\t'elasticnet': {'legend_key':'ElasticNet', 'color':'C2',},\n",
    "\t\t'randomforest': {'legend_key':'RF', 'color':'C4',},\n",
    "\t\t'mlp_chg':{'legend_key':'MLP', 'color':'C5',},\n",
    "\t}\n",
    "\n",
    "\t#region: prepare data for plotting\n",
    "\t#region: create modeling dataset (charge pulse, all SOCs)\n",
    "\t# load pulse and cc data\n",
    "\tcc_data = load_processed_data(data_type='cc')\n",
    "\tpulse_data = load_processed_data(data_type='slowpulse')\n",
    "\t# create modeling data\n",
    "\tall_data = deepcopy(pulse_data)\n",
    "\tidxs = np.where((all_data['pulse_type'] == 'chg'))\n",
    "\tfor k in all_data.keys():\n",
    "\t\tall_data[k] = all_data[k][idxs]\n",
    "\tall_data['voltage_rel'] = np.asarray([v - v[0] for v in all_data['voltage']])\n",
    "\tmodeling_data = create_modeling_data(all_data=all_data, input_feature_keys=['voltage_rel'])\n",
    "\tX = modeling_data['model_input']\n",
    "\ty = modeling_data['model_output'][:,0].reshape(-1,1)\n",
    "\t#endregion\n",
    "\n",
    "\t#region: get CV splits\n",
    "\tcv_splitter = Custom_CVSplitter(n_splits=3, split_type='group_id', rand_seed=random_state)\n",
    "\tcv_splits = list(cv_splitter.split(\n",
    "\t\tX = modeling_data['model_input'], \n",
    "\t\ty = modeling_data['model_output'][:,0], \n",
    "\t\tcell_ids = modeling_data['cell_id']))\n",
    "\t#endregion\n",
    "\t#endregion\n",
    "\n",
    "\tfig, ax = plt.subplots(figsize=(4,2))\n",
    "\tlabels = []\n",
    "\tfor i, model_type in enumerate(list(plot_params.keys())):\n",
    "\t\tfile_study = list(dir_hyperparam_results.glob(f'*{model_type}_onlySOH.pkl*'))[0]\n",
    "\t\tstudy = pickle.load(open(file_study, 'rb'))\n",
    "\t\t# create optimal model from saved parameters values\n",
    "\t\tmodel = None\n",
    "\t\tif model_type == 'ridge': model = Ridge(**study.best_trial.params)\n",
    "\t\telif model_type == 'lasso': model = Lasso(**study.best_trial.params)\n",
    "\t\telif model_type == 'elasticnet': model = ElasticNet(**study.best_trial.params)\n",
    "\t\telif model_type == 'randomforest': model = RandomForestRegressor(**study.best_trial.params)\n",
    "\t\telif model_type == 'mlp_chg': model = _create_mlp_model(**study.best_trial.params, output_shape=(1,))\n",
    "\t\t\n",
    "\t\t# plot avg model error \n",
    "\t\tres = _evaluate_model(model, X, y, cv_splits)\n",
    "\t\t\n",
    "\t\tax.bar(i-0.15, res['train'][0], width=0.3, align='center', color='C0')\n",
    "\t\tax.bar(i+0.15, res['test'][0], width=0.3, align='center', color='C1')\n",
    "\t\t# ax.errorbar(i-0.15, res['train'][0], res['train'][1], capsize=2, color='k')\n",
    "\t\t# ax.errorbar(i+0.15, res['test'][0], res['test'][1], capsize=2, color='k')\n",
    "\t\tlabels.append(plot_params[model_type]['legend_key'])\n",
    "\n",
    "\tax.set_xlabel(\"Model Name\")\n",
    "\tax.set_xticks(np.arange(0,len(labels),1), labels=labels, fontsize=8)\n",
    "\tax.set_ylabel(\"MAPE [%]\")\n",
    "\tax.set_ylim([0,2.8])\n",
    "\tax.set_yticks(np.arange(0,2.8,0.5), np.arange(0,2.8,0.5))\n",
    "\tax.legend(['Train Error', 'Test Error'], ncols=2, loc='upper center', fontsize=8)\n",
    "\tif dir_save is not None:\n",
    "\t\tfilename = dir_save.joinpath(\"ModelPerformanceBaseline_Pulse.pdf\")\n",
    "\t\tfig.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "\tplt.show()\n",
    "\n",
    "dir_save = dir_figures.joinpath(\"raw\", \"Feature Importance\")\n",
    "dir_save.mkdir(exist_ok=True, parents=True)\n",
    "plot_model_performance(\n",
    "\tdir_hyperparam_results = dir_repo_main.joinpath(\"results\", 'hyperparameter_optimization'),\n",
    "\tdir_save = dir_save\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envRapidSOH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
