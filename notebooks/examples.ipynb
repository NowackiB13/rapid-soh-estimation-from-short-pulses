{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/bnowacki/Documents/Git Repositories/rapid-soh-estimation-from-short-pulses')\n",
    "\n",
    "from rapid_soh_estimation.rapid_soh_estimation.config import *\n",
    "from rapid_soh_estimation.rapid_soh_estimation.common_methods import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data = load_processed_data(data_type='cc')\n",
    "pulse_data = load_processed_data(data_type='slowpulse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "\n",
    "def get_cell_ids_in_group(group_id:int) -> np.ndarray:\n",
    "\t\"\"\"Gets all cell ids in the specified group id\n",
    "\n",
    "\tArgs:\n",
    "\t\tgroup_id (int): The id of the group for which to return the cell ids \n",
    "\n",
    "\tReturns:\n",
    "\t\tnp.ndarray: An array of all cell_ids in the specified group\n",
    "\t\"\"\"\n",
    "\tassert group_id in df_test_tracker['Group'].unique(), f\"Invalid group id entered: {group_id}. The group id must be one of the following: {df_test_tracker['Group'].unique()}\"\n",
    "\tstart = (group_id - 1) * 6\n",
    "\tend = min(start+6, 64)\n",
    "\treturn np.arange(start, end, 1) + 2\n",
    "\n",
    "class Custom_CVSplitter():\n",
    "\t\"\"\"A custom cross-validation split wrapper. Allows for splitting by group_id or cell_id and returns n_splits number of cross validation folds\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, n_splits=3, split_type='group_id', rand_seed=None):\n",
    "\t\tassert isinstance(n_splits, int), \"\\'n_splits\\' must be an interger value\"\n",
    "\t\tself.n_splits = n_splits\n",
    "\t\tself.allowed_split_types = ['group_id', 'cell_id']\n",
    "\t\tassert split_type in self.allowed_split_types, \"ValueError. \\'split type\\' must be one of the following: {}\".format(self.allowed_split_types)\n",
    "\t\tself.split_type = split_type\n",
    "\t\tself.rand_seed = rand_seed\n",
    "\t\t\n",
    "\tdef get_n_splits(self, X, y, groups):\n",
    "\t\treturn self.n_splits\n",
    "\n",
    "\tdef split(self, X, y, cell_ids):\n",
    "\t\t'given input data (X) and output data (y), returns (train_idxs, test_idxs) --> idxs are relative to X & y'\n",
    "\t\tkf = None\n",
    "\t\tif self.rand_seed is None:\n",
    "\t\t\tkf = KFold(n_splits=self.n_splits, shuffle=True)\n",
    "\t\telse:\n",
    "\t\t\tkf = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.rand_seed)\n",
    "\t\t\n",
    "\t\tif self.split_type == self.allowed_split_types[0]:      # 'group_id'\n",
    "\t\t\tgroup_ids = np.arange(1, 12, 1)\n",
    "\t\t\t# for every cv split (by group), convert group_id_idxs to X & y idxs\n",
    "\t\t\tfor train_group_idxs, test_group_idxs in kf.split(group_ids):\n",
    "\t\t\t\ttrain_idxs = []\n",
    "\t\t\t\ttest_idxs = []\n",
    "\t\t\t\ttrain_groups = group_ids[train_group_idxs]\n",
    "\t\t\t\ttest_groups = group_ids[test_group_idxs]\n",
    "\t\t\t\t# go through all train group ids in this split\n",
    "\t\t\t\tfor train_group_id in train_groups: \n",
    "\t\t\t\t\ttrain_cell_ids = get_cell_ids_in_group(train_group_id)\n",
    "\t\t\t\t\t# add X & y idxs where cell_id is equal to each cell in this group\n",
    "\t\t\t\t\tfor cell_id in train_cell_ids:\n",
    "\t\t\t\t\t\tcell_idxs = np.hstack(np.argwhere( cell_ids == cell_id ))\n",
    "\t\t\t\t\t\ttrain_idxs.append(cell_idxs)\n",
    "\t\t\t\t# go through all test group ids in this split\n",
    "\t\t\t\tfor test_group_id in test_groups:\n",
    "\t\t\t\t\ttest_cell_ids = get_cell_ids_in_group(test_group_id)\n",
    "\t\t\t\t\t# add X & y idxs where cell_id is equal to each cell in this group\n",
    "\t\t\t\t\tfor cell_id in test_cell_ids:\n",
    "\t\t\t\t\t\tcell_idxs = np.hstack(np.argwhere( cell_ids == cell_id ))\n",
    "\t\t\t\t\t\ttest_idxs.append(cell_idxs)\n",
    "\n",
    "\t\t\t\ttrain_idxs = np.hstack(train_idxs)\n",
    "\t\t\t\ttest_idxs = np.hstack(test_idxs)\n",
    "\t\t\t\tyield train_idxs, test_idxs\n",
    "\t\t\t\t\n",
    "\t\telif self.split_type == self.allowed_split_types[1]:      # 'cell_id'\n",
    "\t\t\t# for every cv split (by cell), convert cell_id_idxs to X & y idxs\n",
    "\t\t\tfor train_cell_idxs, test_cell_idxs in kf.split(np.unique(cell_ids)):\n",
    "\t\t\t\ttrain_idxs = []\n",
    "\t\t\t\ttest_idxs = []\n",
    "\t\t\t\ttrain_cells = np.unique(cell_ids)[train_cell_idxs]\n",
    "\t\t\t\ttest_cells = np.unique(cell_ids)[test_cell_idxs]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# go through all train group ids in this split\n",
    "\t\t\t\tfor train_cell_id in train_cells: \n",
    "\t\t\t\t\tcell_idxs = np.hstack(np.argwhere(cell_ids == train_cell_id))\n",
    "\t\t\t\t\ttrain_idxs.append(cell_idxs)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t# go through all test group ids in this split\n",
    "\t\t\t\tfor test_cell_id in test_cells:\n",
    "\t\t\t\t\tcell_idxs = np.hstack(np.argwhere(cell_ids == test_cell_id))\n",
    "\t\t\t\t\ttest_idxs.append(cell_idxs)\n",
    "\t\t\t\n",
    "\t\t\t\ttrain_idxs = np.hstack(train_idxs)\n",
    "\t\t\t\ttest_idxs = np.hstack(test_idxs)\n",
    "\t\t\t\tyield train_idxs, test_idxs\n",
    "\n",
    "\n",
    "\n",
    "def create_modeling_data(all_data:dict, input_feature_keys:list, output_feature_keys:list=['q_dchg', 'dcir_chg_20', 'dcir_chg_50', 'dcir_chg_90', 'dcir_dchg_20', 'dcir_dchg_50', 'dcir_dchg_90']) -> dict:\n",
    "\t\"\"\"Returns new dictionary with 'model_input' and 'model_output' keys for simpler model training\n",
    "\n",
    "\tArgs:\n",
    "\t\tall_data (dict): All data from which to extrac the specified input and output feature\n",
    "\t\tinput_feature_keys (list): A list of keys (that exist in all_data.keys()) to use as model input\n",
    "\t\toutput_feature_keys (list, optional): A list of keys (that exist in all_data.keys()) to use as model output. Defaults to ['q_dchg', 'dcir_chg_20', 'dcir_chg_50', 'dcir_chg_90', 'dcir_dchg_20', 'dcir_dchg_50', 'dcir_dchg_90'].\n",
    "\n",
    "\tReturns:\n",
    "\t\tdict: A new dict with keys: ['cell_id', 'group_id', 'rpt', 'model_input', 'model_output']\n",
    "\t\"\"\"\n",
    "\tassert len(input_feature_keys) > 0\n",
    "\tfor f in input_feature_keys: assert f in list(all_data.keys())\n",
    "\tfor f in output_feature_keys: assert f in list(all_data.keys())\n",
    "\n",
    "\tmodeling_dic = {\n",
    "\t\t'cell_id':all_data['cell_id'],\n",
    "\t\t'group_id':all_data['group_id'],\n",
    "\t \t'rpt':all_data['rpt'],\n",
    "\t\t'model_input':[],\n",
    "\t\t'model_output':[],\n",
    "\t}\n",
    "\tif len(input_feature_keys) == 1:\n",
    "\t\tmodeling_dic['model_input'] = all_data[input_feature_keys[0]]\n",
    "\n",
    "\tfor i in range(len(all_data['cell_id'])):\n",
    "\t\tif len(input_feature_keys) > 1:\n",
    "\t\t\tmodeling_dic['model_input'].append( [all_data[f_key][i] for f_key in input_feature_keys] )\n",
    "\t\tmodeling_dic['model_output'].append( [all_data[f_key][i] for f_key in output_feature_keys] )\n",
    "\n",
    "\tmodeling_dic['model_input'] = np.asarray(modeling_dic['model_input'])\n",
    "\tmodeling_dic['model_output'] = np.asarray(modeling_dic['model_output'])\n",
    "\treturn modeling_dic\n",
    "\n",
    "def create_model(n_hlayers:int, n_neurons:int, act_fnc:str, opt_fnc:str, learning_rate:float, input_shape=(100,)) -> keras.models.Sequential:\n",
    "\t\"\"\"Builds a Keras neural network model (MLP) using the specified parameters. The model is optimized for accuracy. Make sure model outputs (if multiple target) are normalized, otherwise optimization will be biased towards one target variable.\n",
    "\n",
    "\tArgs:\n",
    "\t\tn_hlayers (int): Number of fully-connected hidden layers\n",
    "\t\tn_neurons (int): Number of neurons per hidden layer\n",
    "\t\tact_fnc (str): Activation function to use (\\'tanh\\', \\'relu\\', etc)\n",
    "\t\topt_fnc (str): {\\'sgd\\', \\'adam\\'} Optimizer function to use \n",
    "\t\tlearning_rate (float): Learning rate\n",
    "\t\tinput_shape (int, optional): Input shape of model. Defaults to (100,).\n",
    "\n",
    "\tRaises:\n",
    "\t\tValueError: _description_\n",
    "\n",
    "\tReturns:\n",
    "\t\tkeras.models.Sequential: compiled Keras model\n",
    "\t\"\"\"\n",
    "\n",
    "\t# add input layer to Sequential model\n",
    "\tmodel = keras.models.Sequential()\n",
    "\tmodel.add( keras.Input(shape=input_shape) )\n",
    "\n",
    "\t# add hidden layers\n",
    "\tfor i in range(n_hlayers):\n",
    "\t\tmodel.add( keras.layers.Dense(units=n_neurons, activation=act_fnc) )\n",
    "\t\t\n",
    "\t# add output layer\n",
    "\tmodel.add( keras.layers.Dense(7) )\n",
    "\n",
    "\t# compile model with chosen metrics\n",
    "\topt = None\n",
    "\tif opt_fnc == 'adam':\n",
    "\t\topt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\telif opt_fnc == 'sgd':\n",
    "\t\topt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\telse:\n",
    "\t\traise ValueError(\"opt_func must be either \\'adam\\' or \\'sgd\\'\")\n",
    "\n",
    "\tmodel.compile(optimizer=opt,\n",
    "\t\t\t\t\tloss=keras.losses.mean_squared_error,      \n",
    "\t\t\t\t\t# make sure to normalize all outputs, otherwise DCIR values will drastically skew MSE reading compared to error of predicted SOH\n",
    "\t\t\t\t\tmetrics=['accuracy'] )\n",
    "\treturn model\n",
    "\n",
    "def get_prediction_error(y_true, y_predicted):\n",
    "    '''returns tuple of (MAPE, RMSE)'''\n",
    "    mape = []\n",
    "    rmse = []\n",
    "    # print(\"y_pred size: \", np.size(y_predicted, axis=1) )\n",
    "    # print(\"y_true size: \", np.size(y_true, axis=1))\n",
    "    if len(np.shape(y_true)) > 1:\n",
    "        for i in range(0, np.size(y_predicted, axis=1)):\n",
    "            mape.append(np.round(mean_absolute_percentage_error(y_true[:,i], y_predicted[:,i]), 4))\n",
    "            rmse.append(np.round(root_mean_squared_error(y_true[:,i], y_predicted[:,i]), 4))\n",
    "    else:\n",
    "        mape.append(np.round(mean_absolute_percentage_error(y_true, y_predicted), 4))\n",
    "        rmse.append(np.round(root_mean_squared_error(y_true, y_predicted), 4))\n",
    "    mape = np.vstack(mape)\n",
    "    rmse = np.vstack(rmse)\n",
    "    return mape.reshape(-1), rmse.reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "# build model\n",
    "model = create_model(5, 100, 'relu', 'sgd', 0.010, input_shape=(100,))\n",
    "\n",
    "# get modeling data\n",
    "all_data = deepcopy(pulse_data)\n",
    "idxs = np.where((all_data['pulse_type'] == 'chg') & (all_data['soc'] == 20))\n",
    "for k in all_data.keys():\n",
    "\tall_data[k] = all_data[k][idxs]\n",
    "all_data['voltage_rel'] = np.asarray([v - v[0] for v in all_data['voltage']])\n",
    "\n",
    "modeling_data = create_modeling_data(all_data=all_data, input_feature_keys=['voltage_rel'])\n",
    "\n",
    "# normalize modeling data inputs and outputs\n",
    "modeling_data['input_scaler'] = StandardScaler()\n",
    "modeling_data['model_input_scaled'] = modeling_data['input_scaler'].fit_transform(modeling_data['model_input'])\n",
    "modeling_data['output_scaler'] = StandardScaler()\n",
    "modeling_data['model_output_scaled'] = modeling_data['output_scaler'].fit_transform(modeling_data['model_output'])\n",
    "    \n",
    "# define early stop callback\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=25, verbose=False, mode='auto', baseline=None, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# run multiple CVs and average estimation error\n",
    "num_iters = 5\n",
    "mape_avgs = []\n",
    "for i in range(num_iters):\n",
    "\t# generate cross validation splits\n",
    "\tcvSplitter = Custom_CVSplitter(n_splits=3, split_type='group_id', rand_seed=i)\n",
    "\tcv_splits = cvSplitter.split(modeling_data['model_input'], modeling_data['model_output'], modeling_data['cell_id']) \n",
    "\tcv_splits = list(cv_splits)\n",
    "\tmape_qdchg = []\n",
    "\tfor cv_idx, (train_idxs, test_idxs) in enumerate(cv_splits):\n",
    "\t\t# get train and test data for this cv split\n",
    "\t\tX_train = modeling_data['model_input_scaled'][train_idxs]\n",
    "\t\ty_train = modeling_data['model_output_scaled'][train_idxs]\n",
    "\t\tX_test = modeling_data['model_input_scaled'][test_idxs]\n",
    "\t\ty_test = modeling_data['model_output_scaled'][test_idxs]\n",
    "\n",
    "\t\t# train model\n",
    "\t\thistory = model.fit(\n",
    "\t\t\tX_train, y_train,\n",
    "\t\t\tvalidation_split = 0.1,\n",
    "\t\t\tbatch_size = 50,\n",
    "\t\t\tepochs = 100,\n",
    "\t\t\tcallbacks = early_stop, \n",
    "\t\t\tverbose = False)\n",
    "\n",
    "\t\ty_pred = model.predict(X_test, verbose=False)\n",
    "\t\terrors = get_prediction_error(y_test, y_pred)\n",
    "\t\t\n",
    "\t\tmape_qdchg.append(errors[0][0])\n",
    "\tprint(f\"Iter {i}: Average MAPE of Q_dchg={round(np.average(np.asarray(mape_qdchg)), 4)}%\")\n",
    "\tmape_avgs.append(np.average(np.asarray(mape_qdchg)))\n",
    "print(f\"Average of all runs: MAPE={round(np.average(np.asarray(mape_avgs)), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# OPTUNA model optimization \n",
    "\n",
    "def optuna_create_model(trial):\n",
    "\tmodel = create_model(\n",
    "\t\tn_hlayers=\t\ttrial.suggest_int(\"n_layers\", 1, 5),\n",
    "\t\tn_neurons=\t\ttrial.suggest_int(\"n_neurons\", 8, 128),\n",
    "\t\tact_fnc=\t\ttrial.suggest_categorical(\"activation\", ['relu', 'tanh']),\n",
    "\t\topt_fnc=\t\ttrial.suggest_categorical(\"optimizer\", ['sgd', 'adam']),\n",
    "\t\tlearning_rate=\ttrial.suggest_float('learning_rate', 1e-5, 1e-1, log=True),\n",
    "\t\tinput_shape=\t(100,)\n",
    "\t)\n",
    "\treturn model\n",
    "\n",
    "def optuna_objective(trial):\n",
    "\ttrain_idxs, test_idxs = cv_splits[0]\n",
    "\tX_train = modeling_data['model_input_scaled'][train_idxs]\n",
    "\ty_train = modeling_data['model_output_scaled'][train_idxs]\n",
    "\tX_test = modeling_data['model_input_scaled'][test_idxs]\n",
    "\ty_test = modeling_data['model_output_scaled'][test_idxs]\n",
    "\t\n",
    "\tmodel = optuna_create_model(trial)\n",
    "\tmodel.fit(\n",
    "\t\tX_train, y_train,\n",
    "\t\tvalidation_split = 0.1,\n",
    "\t\tbatch_size = 50,\n",
    "\t\tepochs = 100,\n",
    "\t\tcallbacks = early_stop, \n",
    "\t\tverbose = False)\n",
    "\t\n",
    "\ty_pred = model.predict(X_test, verbose=False)\n",
    "\terr = get_prediction_error(y_test, y_pred)[0][0]\t# MAPE of q_dchg prediction\n",
    "\treturn err\n",
    "\n",
    "\n",
    "N_TRIALS = 100\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"pulse_model_optimization\")\n",
    "study.optimize(optuna_objective, n_trials=N_TRIALS, n_jobs=-1)\n",
    "print('*'*100)\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"Best Params:\")\n",
    "print(study.best_params)\n",
    "print('*'*100)\n",
    "\n",
    "# get the optimized model\n",
    "opt_model = optuna_create_model(study.best_trial)\n",
    "opt_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envRapidSOH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
